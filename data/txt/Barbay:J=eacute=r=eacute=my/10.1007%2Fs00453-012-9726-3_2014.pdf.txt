Algorithmica (2014) 69:232–268
DOI 10.1007/s00453-012-9726-3

Efﬁcient Fully-Compressed Sequence Representations

Jérémy Barbay · Francisco Claude · Travis Gagie ·
Gonzalo Navarro · Yakov Nekrich

Received: 4 February 2012 / Accepted: 4 December 2012 / Published online: 15 December 2012
© Springer Science+Business Media New York 2012

Abstract We present a data structure that stores a sequence s[1..n] over alphabet 
[1..σ] in nH0(s) + o(n)(H0(s)+1) bits, where H0(s) is the zero-order entropy
of s. This structure supports the queries access, rank and select, which are fundamental 
building blocks for many other compressed data structures, in worst-case
time O(lg lg σ ) and average time O(lgH0(s)). The worst-case complexity matches
the best previous results, yet these had been achieved with data structures using
nH0(s) + o(n lg σ ) bits. On highly compressible sequences the o(n lg σ ) bits of the
redundancy may be signiﬁcant compared to the nH0(s) bits that encode the data. Our
representation, instead, compresses the redundancy as well. Moreover, our averagecase 
complexity is unprecedented.

An early version of this article appeared in Proc. 21st Annual International Symposium on
Algorithms and Computation (ISAAC), part II, pp. 215–326, 2010. J. Barbay was funded in part by
Fondecyt grants 1-110066 and 1120054, Chile. F. Claude was funded in part by Google PhD
Fellowship Program. G. Navarro was funded in part by Fondecyt grant 1-110066, Chile. Work
partially done during Y. Nekrich stay at the University of Chile.
J. Barbay · G. Navarro ((cid:2))
Department of Computer Science, University of Chile, Santiago, Chile
e-mail: gnavarro@dcc.uchile.cl

J. Barbay
e-mail: jbarbay@dcc.uchile.cl

F. Claude
David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada
e-mail: fclaude@cs.uwaterloo.ca

T. Gagie
Department of Computer Science, Aalto University, Helsinki, Finland
e-mail: travis.gagie@gmail.com

Y. Nekrich
Department of Electrical Engineering & Computer Science, University of Kansas, Kansas, USA
e-mail: yasha@dcc.uchile.cl

Algorithmica (2014) 69:232–268

233

Our technique is based on partitioning the alphabet into characters of similar frequency.
 The subsequence corresponding to each group can then be encoded using fast
uncompressed representations without harming the overall compression ratios, even
in the redundancy.

The result also improves upon the best current compressed representations of several 
other data structures. For example, we achieve (i) compressed redundancy, retaining 
the best time complexities, for the smallest existing full-text self-indexes;
−1() improved to logloga-
(ii) compressed permutations π with times for π() and π
rithmic; and (iii) the ﬁrst compressed representation of dynamic collections of disjoint 
sets. We also point out various applications to inverted indexes, sufﬁx arrays,
binary relations, and data compressors.

Our structure is practical on large alphabets. Our experiments show that, as predicted 
by theory, it dominates the space/time tradeoff map of all the sequence representations,
 both in synthetic and application scenarios.
Keywords Compressed sequence representations · Rank and select on sequences ·
Compact data structures · Entropy-bounded structures · Compressed text indexing

1 Introduction

A growing number of important applications require data representations that are
space-efﬁcient and at the same time support fast query operations. In particular, suitable 
representations of sequences supporting a small set of basic operations yield
spaceand 
time-efﬁcient implementations for many other data structures such as fulltext 
indexes [22, 28, 32, 48], labeled trees [3, 4, 20], binary relations [2, 4], permutations 
[6] and two-dimensional point sets [11, 41], to name a few.
Let s[1..n] be a sequence of characters belonging to alphabet [1..σ]. In this article
we focus on the following set of operations, which is sufﬁcient for many applica-
tions:
s.access(i) returns the ith character of sequence s, which we denote s[i];
s.ranka(i)

returns the number of occurrences of character a up to position i in s;
and

s.selecta(i) returns the position of the ith occurrence of a in s.
Table 1 shows the best sequence representations and the complexities they achieve
for the three queries, where Hk(s) refers to the k-th order empirical entropy of s [43].
To implement the operations efﬁciently, the representations require some redundancy
space on top of the nH0(s) or nHk(s) bits needed to encode the data. For example,
multiary wavelet trees (row 1) represent s within zero-order entropy space plus just
o(n) bits of redundancy, and support queries in time O(1 + lg σ
lg lg n ). This is very attractive 
for relatively small alphabets, and even constant-time for polylog-sized ones.
For large σ , however, all the other representations in the table are exponentially faster,
and some even achieve high-order compression. However, their redundancy is higher,
o(n lg σ ) bits. While this is still asymptotically negligible compared to the size of a
plain representation of s, on highly compressible sequences such redundancy is not
always negligible compared to the space used to encode the compressed data. This

234

Algorithmica (2014) 69:232–268

Table 1 Best previous bounds and our new ones for data structures supporting access, rank and select.
The space bound of the form Hk (s) holds for any k = o(logσ n), and those of the form (1 + ) hold for
any constant  > 0. On average lg σ becomes H0(s) in our time complexities (see Corollary 3) and in row
1 [7, Theorem 5]

[29, Theorem 4]
[4, Lemma 4.1]
[33, Corollary 2]
[28, Theorem 2.2]

Theorem 2
Theorem 2
Corollary 4

space (bits)

nH0(s) + o(n)
nH0(s) + o(n lg σ )
nHk (s) + o(n lg σ )
(1 + )n lg σ
nH0(s) + o(n)(H0(s) + 1)
nH0(s) + o(n)(H0(s) + 1)
(1 + )nH0(s) + o(n)

(cid:3)

lg lg n

access
O(cid:2)
1 + lg σ
O(lg lg σ )
O(1)
O(1)
O(lg lg σ )
O(1)
O(1)

(cid:3)

lg lg n

rank
O(cid:2)
1 + lg σ
O(lg lg σ )
O(lg lg σ )
O(lg lg σ )
O(lg lg σ )
O(lg lg σ )
O(lg lg σ )

(cid:3)

1 + lg σ

lg lg n

select
O(cid:2)
O(1)
O(lg lg σ )
O(1)
O(1)
O(lg lg σ )
O(1)

raises the challenge of retaining the efﬁcient support for the queries while compressing 
the index redundancy as well.

In this paper we solve this challenge in the case of zero-order entropy compression,
 that is, the redundancy of our data structure is asymptotically negligible compared 
to the zero-order compressed text size (not only compared to the plain text
size), plus o(n) bits. The worst-case time our structure achieves is O(lg lg σ ), which
matches the best previous results for large σ . Moreover, the average time is logarithmic 
on the entropy of the sequence, O(lgH0(s)), under reasonable assumptions on
the query distribution. This average time complexity is also unprecedented: the only
previous entropy-adaptive time complexities we are aware of come from Huffmanshaped 
wavelet trees [32], which have recently been shown capable of achieving
O(1 + H0(s)

lg lg n ) query time with just o(n) bits of redundancy [7, Theorem 5].

Our technique is described in Sect. 3. It can be summarized as partitioning the
alphabet into sub-alphabets that group characters of similar frequency in s, storing
in a multiary wavelet tree [22] the sequence of sub-alphabet identiﬁers, and storing
separate sequences for each sub-alphabet, containing the subsequence of s formed
by the characters of that sub-alphabet. Golynski et al.’s [28] or Grossi et al.’s [33]
structures are used for these subsequences, depending on the tradeoff to be achieved.
We show that it is sufﬁcient to achieve compression in the multiary wavelet tree,
while beneﬁtting from fast operations on the representations of the subsequences.

The idea of alphabet partitioning is not new. It has been used in practical scenarios 
such as fax encoding, JPEG and MPEG formats [36, 52], and in other image
coding methods [51], with the aim of speeding up decompression: only the (short)
sub-alphabet identiﬁer is encoded with a sophisticated (and slow) method, whereas
the sub-alphabet characters are encoded with a simple and fast encoder (even in plain
form). Said [59] gave a more formal treatment to this concept, and designed a dynamic 
programming algorithm to ﬁnd the optimal partitioning given the desired number 
of sub-alphabets, that is, the one minimizing the redundancy with respect to the
zero-order entropy of the sequence. He proved that an optimal partitioning deﬁnes
sub-alphabets according to ranges of character frequencies, which reduces the cost of

Algorithmica (2014) 69:232–268

235

ﬁnding such partitioning to polynomial time and space (more precisely, quadratic on
the alphabet size).

Our contribution in this article is, on one hand, to show that a particular way to
deﬁne the sub-alphabets, according to a quantization of the logarithms of the inverse
probabilities of the characters, achieves o(H0(s) + 1) bits of redundancy per character 
of the sequence s. This value, in particular, upper bounds the coding efﬁciency of
Said’s optimal partitioning method. On the other hand, we apply the idea to sequence
data structures supporting operations access/rank/select, thus achieving efﬁcient support 
of indexed operations on the sequence, not only fast decoding.

We also consider various extensions and applications of our main result. In Sect. 4
we show how our result can be used to improve an existing text index that achieves
k-th order entropy [4, 22], so as to improve its redundancy and query times. In this
way we achieve the ﬁrst self-index with space bounded by nHk(s)+ o(n)(Hk(s)+ 1)
bits, for any k = o(logσ n), able to count and locate pattern occurrences and extract

any segment of s within the time complexities achieved by its fastest predecessors.
We also achieve new space/time tradeoffs for inverted indexes and binary relations.
In Sects. 5 and 6 we show how to apply our data structure to store a compressed
permutation and a compressed function, respectively, supporting direct and inverse
applications and in some cases improving upon previous results [6, 7, 38, 47]. We
describe further applications to text indexes and binary relations. In particular, an application 
of our structure on permutations, at the end of Sect. 5, achieves for the ﬁrst
time compressed redundancy to store function Ψ of text indexes [32, 35, 57]. Section 
7 shows how to maintain a dynamic collection of disjoint sets, while supporting
operations union and ﬁnd, in compressed form. This is, to the best of our knowledge,
the ﬁrst result of this kind.

2 Related Work
Sampling A basic attempt to provide rank and select functionality on a sequence
s[1..n] over alphabet [1..σ] is to store s in plain form and the values s.ranka(k · i) for
all a ∈ [1..σ] and i ∈ [1..n/ k], where k ∈ [1..n] is a sampling parameter. This yields
constant-time access, O(k/ logσ n) time rank, and O(k/ logσ n + lg lg n) time select
if we process Θ(logσ n) characters of s in constant time using universal tables, and
organize the rank values for each character in predecessor data structures. The total
space is n lg σ + O((n/ k)σ lg n). For example, we can choose k = σ lg n to achieve
total space n lg σ + O(n) (that is, the data plus the redundancy space). Within this
space we can achieve time complexity O(σ lg σ ) for rank and O(σ lg σ + lg lg n) for
select.

Succinct Indexes The previous construction separates the sequence data from the
“index”, that is, the extra data structures to provide fast rank and select. There are
much more sophisticated representations for the sequence data that offer constanttime 
access to Θ(logσ n) consecutive characters of s (i.e., just as if s were stored
in plain form), yet achieving nHk(s) + o(n lg σ ) bits of space, for any k = o(logσ n)
[23, 31, 58]. We recall that Hk(s) is the k-th order empirical entropy of s [43], a lower
bound to the space achieved by any statistical compressor that models the character 
probabilities using the context of their k preceding characters, so 0 ≤ Hk(s) ≤

236

Algorithmica (2014) 69:232–268

Hk−1(s) ≤ H0(s) ≤ lg σ . Combining such sequence representations with sophisticated 
indexes that require o(n lg σ ) bits of redundancy [4, 33] (i.e., they are “suc-
cinct”), we obtain results like row 3 of Table 1.
Bitmaps A different alternative is to maintain one bitmap ba[1..n] per character
a ∈ [1..σ], marking with a 1 the positions i where s[i] = a. Then s.ranka(i) =
ba.rank1(i) and s.selecta(j ) = ba.select1(j ). The bitmaps can be represented in compressed 
form using “Fully Indexable Dictionaries” (FIDs) [55], so that they operate 
in constant time and the total space is nH0(s) + O(n) + o(σ n) bits. Even with
space-optimal FIDs [53, 54], this space is nH0(s)+O(n)+O( σ n
lgc n ) (and the time is
O(c)) for any constant c, which is acceptable only for polylog-sized alphabets, that
is, σ = O(polylog(n)). An alternative is to use weaker compressed bitmap representations 
[35, 50] that can support select1 in constant time and rank1 in time O(lg n),
and yield an overall space of nH0(s) + O(n) bits. This can be considered as a succinct 
index over a given sequence representation, or we can note that we can actually
solve s.access(i) by probing all the bitmaps ba.access(i). Although this takes at
least O(σ ) time, it is a simple illustration of another concept: rather than storing an
independent index on top of the data, the data is represented in a way that provides
access, rank and select operations with reasonable efﬁciency.

Wavelet Trees The wavelet tree [32] is a structure integrating data and index, that
provides more balanced time complexities. It is a balanced binary tree with one leaf
per alphabet character, and storing bitmaps in its internal nodes, where constanttime 
rank and select operations are supported. By using FIDs [55] to represent those
bitmaps, wavelet trees achieve nH0(s) + O( n lg σ lg lg n
) bits of space and support all
three operations in time proportional to their height, O(lg σ ). Multiary wavelet trees
[22] replace the bitmaps by sequences over sublogarithmic-sized alphabets [1..σ
(cid:4)],
(cid:4) = O(lg n) for 0 <  < 1, in order to reduce that height. The FID technique is
σ
extended to alphabets of those sizes while retaining constant times. Multiary wavelet
trees obtain the same space as the binary ones, but their time complexities are reduced 
by an O(lg lg n) factor. Indeed, if σ is small enough, σ = O(polylog(n)), the
tree height is a constant and so are all the query times. Recently, the redundancy of
multiary (and binary) wavelet trees has been reduced to just o(n) [29], which yields
the results in the ﬁrst row of Table 1.1

lg n

Huffman-Shaped Wavelet Trees Another alternative to obtain zero-order compression 
is to give Huffman shape to the wavelet tree [32]. This structure uses nH0(s) +
o(nH0(s)) + O(n) bits even if the internal nodes use a plain representation, using
|b| + o(|b|) bits [13, 46], for their bitmaps b. Limiting the height to O(lg σ ) retains
the worst-case times of the balanced version and also the given space [7]. In order to
reduce the time complexities by an O(lg lg n) factor, we can build multiary wavelet

1Because of these good results on polylog-sized alphabets, we focus on larger alphabets in this article, and
therefore do not distinguish between redundancies of the form o(n) lg σ and no(lg σ ), writing o(n lg σ ) for
all. See also footnote 6 of Barbay et al. [4].

Algorithmica (2014) 69:232–268

237

trees over multiary Huffman trees [39]. This can be combined with the improved representation 
for sequences over small alphabets [29] so as to retain the nH0(s)+ o(n)
bits of space and O(1 + lg σ
lg lg n ) worst-case times of balanced multiary wavelet trees.
The interesting aspect of using Huffman-shaped trees is that, if the access queries distribute 
uniformly over the text positions, and the character arguments a to ranka and
selecta are chosen according to their frequency in s, then the average time complexities 
are O(1 + H0(s)
lg lg n ), the weighted leaf depth. This result [7, Theorem 5] improves
upon the multiary wavelet tree representation [29] in the average case. We note that
this result [7] involves O(σ lg n) extra bits of space redundancy, which is negligible
only for σ = o(n/ lg n).

−1() in time O(1/) is used. Depending on whether π or π

Reducing to Permutations A totally different sequence representation [28] improves
the times to poly-loglogarithmic on σ , that is, exponentially faster than multiary
wavelet trees when σ is large enough. Yet, this representation requires again uncompressed 
space, n lg σ + O( n lg σ
lg lg σ ).2 It cuts the sequence into chunks of length σ
and represents each chunk using a permutation π (which acts as an inverted index of
−1() are needed, a reprethe 
characters in the chunk). As both operations π() and π
sentation [47] that stores the permutation within (1+ )σ lg σ bits and computes π()
−1
in constant time and π
is represented explicitly, constant time is achieved for select or for access. Using a
constant value for  yields a slightly larger representation that solves both access
and select in constant time. Later, the space of this representation was reduced to
nH0(s) + o(n lg σ ) bits while retaining the time complexities of one of the variants
(constant-time select) [4]. In turn, the variant offering constant-time access was superseded 
by the index of Grossi et al. [33], which achieves high-order compression
and also improves upon a slower alternative that takes the same space [4]. The best
current times are either constant or O(lg lg σ ). We summarize them in rows 2 to 4.
Our contribution, in rows 5 to 7 of Table 1, is to retain times loglogarithmic on σ ,
as in rows 2 to 4, while compressing the redundancy space. This is achieved only with
space H0(s), not Hk(s). We also achieve average times depending on H0(s) instead
of lg σ .

3 Alphabet Partitioning
Let s[1..n] be a sequence over effective alphabet [1..σ].3 We represent s using an
alphabet partitioning scheme. Our data structure has three components:
1. A character mapping m[1..σ] that separates the alphabet into sub-alphabets. That

is, m is the sequence assigning to each character a ∈ [1..σ] the sub-alphabet

m[a] = (cid:4)

lg(n/|s|a) lg n

(cid:5)
,

2The representation actually compresses to the k-th order entropy of a different sequence, not s (A. Golynski,
 personal communication).
3By effective we mean that every character appears in s, and thus σ ≤ n. In Sect. 3.3 we handle the case
of larger alphabets.

238

Algorithmica (2014) 69:232–268

where |s|a denotes the number of occurrences of character a in s; note that m[a] ≤
(cid:5)lg2 n(cid:6) for any a ∈ [1..σ].
2. The sequence t[1..n] of the sub-alphabets assigned to each character in s. That is,
t is the sequence over [1..(cid:5)lg2 n(cid:6)] obtained from s by replacing each occurrence
of a by m[a], namely t[i] = m[s[i]].
3. The subsequences s(cid:7)[1..σ(cid:7)] of characters of each sub-alphabet. For 0 ≤ (cid:7) ≤
(cid:5)lg2 n(cid:6), let σ(cid:7) = |m|(cid:7), that is, the number of distinct characters of s replaced by (cid:7)
in t. Then s(cid:7)[1..|t|(cid:7)] is the sequence over [1..σ(cid:7)] deﬁned by

(cid:6)
t.rank(cid:7)(i)

(cid:7) = m.rank(cid:7)

s[i](cid:3)
(cid:2)

,

s(cid:7)

for all 1 ≤ i ≤ n such that t[i] = (cid:7).

Example 1 Let s = "alabar a la alabarda". Then n = 20 and |s|a = 9,
|s|l = |s|" = 3, |s|b = |s|r = 2, and |s|d = 1. Accordingly, we deﬁne the mapping 
as m[a] = 5, m[l] = m["] = 12, m[b] = m[r] = 15, and m[d] = 19. As
this is the effective alphabet, and assuming that the order is "",a,b,d,l,r",
we have m = (12, 5, 15, 19, 12, 15). So the sequence of sub-alphabet identiﬁers
is t[1..20] = (5, 12, 5, 15, 5, 15, 12, 5, 12, 12, 5, 12, 5, 12, 5, 15, 5, 15, 19, 5),
and the subsequences are s5 = (1, 1, 1, 1, 1, 1, 1, 1, 1), s12 = (2, 1, 1, 2, 1, 2),
s15 = (1, 2, 1, 2), and s19 = (1).

With these data structures we can implement the queries on s as follows:
(cid:3)(cid:3)
, where (cid:7) = t.access(i);

(cid:2)
(cid:2)
s(cid:7).access
t.rank(cid:7)(i)
(cid:3)
, where (cid:7) = m.access(a) and c = m.rank(cid:7)(a);
t.rank(cid:7)(i)
(cid:2)
(cid:3)
s(cid:7).selectc(i)

, where (cid:7) = m.access(a) and c = m.rank(cid:7)(a).

s.access(i) = m.select(cid:7)
(cid:2)
s.ranka(i) = s(cid:7).rankc
s.selecta(i) = t.select(cid:7)

Example 2 In the representation of Example 1, we solve s.access(6) by ﬁrst
computing (cid:7) = t.access(6) = 15 and then m.select15(s15.access(t.rank15(6))) =
m.select15(s15.access(2)) = m.select15(2) = r. Similarly,
to solve s.rankl(14)
we compute (cid:7) = m.access(l) = 12 and c = m.rank12(l) = 2. Then we return 
s12.rank2(t.rank12(14)) = s12.rank2(6) = 3. Finally, to solve s.selectr(2), we
compute (cid:7) = m.access(r) = 15 and c = m.rank15(r) = 2,
and return
t.select15(s15.select2(2)) = t.select15(4) = 18.

3.1 Space Analysis
Recall that the zero-order entropy of s[1..n] is deﬁned as
n|s|a

H0(s) = (cid:8)
a∈[1..σ]

|s|a

lg

n

.

(1)

Recall also that, by convexity, nH0(s) ≥ (σ − 1) lg n + (n − σ + 1) lg
next lemma gives the key result for the space analysis.

n

n−σ+1 . The

Algorithmica (2014) 69:232–268

Lemma 1 Let s, t, σ(cid:7) and s(cid:7) be as deﬁned above. Then nH0(t ) + (cid:9)
nH0(s) + o(n).
Proof First notice that, for any character 1 ≤ (cid:7) ≤ (cid:5)lg2 n(cid:6) it holds that

239

|s(cid:7)| lg σ(cid:7) ∈

(cid:7)

(cid:8)

|s|c = |s(cid:7)|.

(2)

c,(cid:7)=m[c]
Now notice that, if m[a] = m[b] = (cid:7), then
lg(n/|s|a) lg n

(cid:5) = (cid:4)

lg(n/|s|b) lg n

(cid:7) = (cid:4)
therefore lg(n/|s|b) − lg(n/|s|a) < 1/ lg n,
and so |s|a < 21/ lg n|s|b.

(cid:5)

,

(3)
Now, ﬁx a, call (cid:7) = m[a], and sum Eq. (3) over all those b such that m[b] = (cid:7). The
second step uses Eq. (2):

(cid:8)

b,(cid:7)=m[b]

(cid:8)

|s|a <
σ(cid:7)|s|a < 21/ lg n|s(cid:7)|,

b,(cid:7)=m[b]

21/ lg n|s|b,

σ(cid:7) < 21/ lg n|s(cid:7)|/|s|a.

|s(cid:7)| = n, we have, using Eq. (1), (2), and (4),
|s(cid:7)| lg σ(cid:7)

Since

(cid:9)
a

(cid:7)

|s|a = (cid:9)
nH0(t ) + (cid:8)
= (cid:8)
(cid:7)
(cid:8)

(cid:7)

(cid:8)

(4)

(cid:3)

|s(cid:7)| lg(n/|s(cid:7)|) + (cid:8)

(cid:8)

(cid:7)

a,(cid:7)=m[a]
|s|a lg(n/|s(cid:7)|) + (cid:8)
|s|a lg(n/|s|a) + (cid:8)

(cid:7)

a,(cid:7)=m[a]
|s|a lg(n/|s|a) + n/ lg n

(cid:7)

<

a,(cid:7)=m[a]

(cid:7)

(cid:7)

(cid:8)

= (cid:8)
= (cid:8)
∈ nH0(s) + o(n).

a

|s|a lg σ(cid:7)
(cid:8)

a,(cid:7)=m[a]
(cid:8)

a,(cid:7)=m[a]

(cid:2)
21/ lg n|s(cid:7)|/|s|a

|s|a lg
|s|a/ lg n

(cid:2)
In other words, if we represent t with H0(t ) bits per character and each s(cid:7) with
lg σ(cid:7) bits per character, we achieve a good overall compression. Thus we can obtain
a very compact representation of a sequence s by storing a compact representation of
t and storing each s(cid:7) as an “uncompressed” sequence over an alphabet of size σ(cid:7).

240

Algorithmica (2014) 69:232–268

3.2 Concrete Representation

lg n

lg n

|s(cid:7)| lg σ(cid:7)
lg lg σ(cid:7)

lg n ) · H0(s).
Therefore we have nH0(t ) + o(n) bits for t,

We represent t and m as multiary wavelet trees [22]; we represent each s(cid:7) as either
a multiary wavelet tree or an instance of Golynski et al.’s [28, Theorem 2.2] ac-
cess/rank/select data structure, depending on whether σ(cid:7) ≤ lg n or not. The wavelet
tree for t uses at most nH0(t ) + O( n(lg lg n)2
) bits and operates in constant time,
because its alphabet size is polylogarithmic (i.e., (cid:5)lg2 n(cid:6)). If s(cid:7) is represented as a
|s(cid:7)| lg σ(cid:7) lg lg n
wavelet tree, it uses at most |s(cid:7)|H0(s(cid:7)) + O(
) bits4 and again operates in
constant time because σ(cid:7) ≤ lg n; otherwise it uses at most |s(cid:7)| lg σ(cid:7) + O(
) ≤
|s(cid:7)| lg σ(cid:7)
|s(cid:7)| lg σ(cid:7)+O(
lg lg lg n ) bits (the latter because σ(cid:7) > lg n). Thus in either case the space
|s(cid:7)| lg σ(cid:7)
for s(cid:7) is bounded by |s(cid:7)| lg σ(cid:7) + O(
lg lg lg n ) bits. Finally, since m is a sequence of
length σ over an alphabet of size (cid:5)lg2 n(cid:6), the wavelet tree for m takes O(σ lg lg n)
bits and also operates in constant time. Because of the convexity property we referred 
to in the beginning of this section, nH0(s) ≥ (σ − 1) lg n, the space for m is
O( n lg lg n
lg lg lg n )) bits
for the s(cid:7) sequences, and o(n)H0(s) bits for m. Using Lemma 1, this adds up to
nH0(s) + o(n)H0(s) + o(n), where the o(n) term is O(
Using the variant of Golynski et al.’s data structure [28, Theorem 4.2], that gives
constant-time select, and O(lg lg σ ) time for rank and access, we obtain our ﬁrst
result in Table 1(row 4). To obtain our second result (row 5), we use instead Grossi et
al.’s result [33, Corollary 2], which gives constant-time access, and O(lg lg σ ) time
|s(cid:7)| lg σ(cid:7)
for rank and select. We note that their structure takes space |s(cid:7)|Hk(s(cid:7)) + O(
),
lg lg σ(cid:7)
yet we only need this to be at most |s(cid:7)| lg σ(cid:7) + O(
Theorem 2 We can store s[1..n] over effective alphabet [1..σ] in nH0(s) +
o(n)(H0(s) + 1) bits and support access, rank and select queries in O(lg lg σ ),
O(lg lg σ ), and O(1) time, respectively (variant (i)); or in O(1), O(lg lg σ ) and
O(lg lg σ ) time, respectively (variant (ii)).

|s(cid:7)| lg σ(cid:7)(1 + O(

|s(cid:7)| lg σ(cid:7)
lg lg lg n ).

lg lg lg n ).

(cid:9)
(cid:7)

1

n

We can reﬁne the time complexity by noticing that the only non-constant times
are due to operating on some sequence s(cid:7), where the alphabet is of size σ(cid:7) <
21/ lg n|s(cid:7)|/|s|a, where a is the character in question, thus lg lg σ(cid:7) = O(lg lg(n/|s|a)).
If we assume that the characters a used in queries distribute with the same frequencies
as in sequence s (e.g., access queries refer to randomly chosen positions in s), then
) = O(lgH0(s)) by the log-sum
the average query time becomes O(
inequality.5

|s|a
n lg lg n|s|a

(cid:9)

a

4This is achieved by using block sizes of length lg n
√
of size O(
will be asymptotic in n.
5Given σ pairs of numbers ai , bi > 0, it holds that
bi = −ai lg ai to obtain the result.

, at the price of storing universal tables
npolylog(n)) = o(n) bits. Therefore all of our o(·) expressions involving n and other variables
. Use ai = |s|i /n and

ai lg ai
bi

(cid:9)
ai(cid:9)
bi

≥ (

ai ) lg

(cid:9)

(cid:9)

|

2 and not lg|s(cid:7)

2

Algorithmica (2014) 69:232–268

241

Corollary 3 The O(lg lg σ ) time complexities in Theorem 2 are also O(lg lg(n/|s|a)),
where a stands for s[i] in the access query, and for the character argument in the
ranka and selecta queries. If these characters a distribute on queries with the same
frequencies as s, the average time complexity for those operations is O(lgH0(s)).

Finally, to obtain our last result in Table 1 we use again Golynski et al.’s representation 
[28, Theorem 4.2]. Given |s(cid:7)| lg σ(cid:7) extra space to store the inverse of a
permutation inside chunks, it answers select queries in time O(1) and access queries
in time O(1/) (these two complexities can be interchanged), and rank queries in
time O(lg lg σ(cid:7)). While we initially considered 1/ = lg lg σ(cid:7) to achieve the main
result, using a constant  yields constant-time select and access simultaneously.
Corollary 4 We can store s[1..n] over effective alphabet [1..σ] in (1 + )nH0(s) +
o(n) bits, for any constant  > 0, and support access, ranka and select queries in
O(1/), O(lg lg min(σ, n/|s|a)), and O(1) time, respectively (variant (i)); or in O(1),
O(lg lg min(σ, n/|s|a)), and O(1/), respectively (variant (ii)).

3.3 Handling Arbitrary Alphabets

(cid:4)

(cid:4)

(cid:4)

(cid:4)

with its rank in Σ

the answer is −1); and for any i ∈ [1..σ] the i-th smallest element in Σ

In the most general case, s is a sequence over an alphabet Σ that is not an effective
be the set of elements that
alphabet, and σ characters from Σ occur in s. Let Σ
to elements of [1..σ] by replacing each
(cid:4)
occur in s; we can map characters from Σ
a ∈ Σ
(cid:4)
. All elements of Σ
are stored in the “indexed dictionary”
(ID) data structure described by Raman et al. [55], so that the following queries are
supported in constant time: for any a ∈ Σ
(cid:4)
can be found (for any
a /∈ Σ
can
be found. The ID structure uses σ lg(eμ/σ )+ o(σ )+O(lg lg μ) bits of space, where
; the value
e is the base of the natural logarithm and μ is the maximal element in Σ
of μ can be speciﬁed with additional O(lg μ) bits. We replace every element in s by
its rank in Σ
, and the resulting sequence is stored using Theorem 2. Hence, in the
general case the space usage is increased by σ lg(eμ/σ ) + o(σ ) + O(lg μ) bits and
the asymptotic time complexity of queries remains unchanged. Since we are already
spending O(σ lg lg n) bits in our data structure, this increases the given space only by
O(σ lg(μ/σ )).

its rank in Σ

(cid:4)

(cid:4)

(cid:4)

(cid:4)

3.4 Application to Fast Encode/Decode

Given a sequence s to encode, we can build mapping m from its character frequencies
|s|a, and then encode each s[i] as the pair (m[s[i]], m.rankm[s[i]](s[i])). Lemma 1
(and some of the discussion that follows in Sect. 3.2) shows that the overall output
size is nH0(s)+ o(n) bits if we represent the sequence of pairs by partitioning it into
three sequences: (1) the left part of the pairs in one sequence, using Huffman coding
on chunks (see next); (2) the right part of the pairs corresponding to values where
σ(cid:7) < lg n in a second sequence, using Huffman coding on chunks; (3) the remaining
right parts of the pairs, using plain encoding in (cid:5)lg σ(cid:7)(cid:6) bits (note σ(cid:7) = m.rank(cid:7)(σ )).
lg n
The Huffman coding on chunks groups
4 lg lg n characters, so that even in the case of

242

Algorithmica (2014) 69:232–268

√
2 bits, and hence the Huffman coding table occupies just O(

the left parts, where the alphabet is of size (cid:5)lg2 n(cid:6), the total length of a chunk is at
most lg n
n lg n) bits. The
redundancy on top of H0(s) adds up to O( n lg lg n
lg n ) bits in sequences (1) and (2) (one
bit of Huffman redundancy per chunk) and O( n
lg lg n ) in sequence (3) (one bit, coming
from the ceil function, per lg σ(cid:7) > lg lg n encoded bits).
The overall encoding time is O(n). A pair ((cid:7), o) is decoded as s[i] = m.select(cid:7)(o),
where after reading (cid:7) we can compute σ(cid:7) to determine whether o is encoded in sequence 
(2) or (3). Thus decoding also takes constant time if we can decode Huffman
codes in constant time. This can be achieved by using canonical codes and limiting
the height of the tree [24, 45].
This construction gives an interesting space/time tradeoff with respect to classical 
alternatives. Using just Huffman coding yields O(n) encoding/decoding time,
but only guarantees nH0(s) + O(n) bits of space. Using arithmetic coding achieves
nH0(s) + O(1) bits, but encoding/decoding is not linear-time. The tradeoff given by
our encoding, nH0(s)+ o(n) bits and linear-time decoding, is indeed the reason why
it is used in practice in various folklore applications, as mentioned in the Introduction.
In Sect. 8.2 we experimentally evaluate these ideas and show they are practical. Next,
we give more far-fetched applications of the rank/select capabilities of our structure,
which go much beyond the mere compression.

4 Applications to Text Indexing

Our main result can be readily carried over various types of indexes for text collections.
 These include self-indexes for general texts, and positional and non-positional
inverted indexes for natural language text collections.

4.1 Self-Indexes

A self-index represents a sequence and supports operations related to text searching
on it. A well-known self-index [22] achieves k-th order entropy space by partitioning
the Burrows-Wheeler transform [12] of the sequence and encoding each partition
to its zero-order entropy. Those partitions must support queries access and rank.
By using Theorem 2(i) to represent such partitions, we achieve the following result,
improving previous ones [4, 22, 28].
Theorem 5 Let s[1..n] be a sequence over effective alphabet [1..σ]. Then we can
represent s using nHk(s) + o(n)(Hk(s) + 1) bits, for any k ≤ (δ logσ n) − 1 and
constant 0 < δ < 1, while supporting the following queries:
(i) count the number of occurrences of a pattern p[1..m] in s, in time O(m lg lg σ );
(ii) locate any such occurrence in time O(lg n lg lg lg n lg lg σ );
(iii) extract s[l, r] in time O((r − l) lg lg σ + lg n lg lg lg n lg lg σ ).
Proof To achieve nHk(s) space, the Burrows-Wheeler transformed text sbwt is partitioned 
into r ≤ σ k sequences s1 ··· sr [22]. Since k ≤ (δ logσ n) − 1, it follows

Algorithmica (2014) 69:232–268

243

|si|H0(si ) + (H0(si ) + 1) · O(

that σ k+1 ≤ nδ. The space our Theorem 2(i) achieves using such a partition is
(cid:9)
|si|
lg lg lg|si| ). Let γ = (1 − δ)/2 (so 0 < δ + γ < 1
whenever 0 < δ < 1) and classify the sequences si according to whether |si| < nγ
i
(short sequences) or not (long sequences). The total space occupied by the short sequences 
can be bounded by r · O(nγ lg σ ) = O(nδ+γ ) = o(n) bits. In turn, the space
lg lg lg n ) · |si|H0(si ) +
occupied by the long sequences can be bounded by
d|si|
lg lg lg n bits, for some constants c, d. An argument very similar to the one used by
Ferragina et al. [22, Theorem 4.2] shows that these add up to (1+ c
lg lg lg n )· nHk(s)+
lg lg lg n . Thus the space is nHk(s) + o(n)(Hk(s) + 1). Other structures required by
the alphabet partitioning technique [22] add o(n) more bits if σ k+1 ≤ nδ.

(cid:9)
i (1 + c

dn

n

The claimed time complexities stem from the rank and access times on the partitions.
 The partitioning scheme [22] adds just constant time overheads. Finally,
to achieve the claimed locating and extracting times we sample one out of every
lg n lg lg lg n text positions. This maintains our lower-order space term o(n) within
O(
(cid:2)
lg lg lg n ).
In case [1..σ] is not the effective alphabet we proceed as described in Sect. 3.3.
Our main improvement compared to Theorem 4.2 of Barbay et al. [4] is that we
have compressed the redundancy from o(n lg σ ) to o(n)(Hk(s) + 1). Our improved
locating times, instead, just owe to the denser sampling, which Barbay et al. could
also use.
Note that, by using the zero-order representation of Golynski et al. [29, Theorem 
4], we could achieve even better space, nHk(s) + o(n) bits, and time complexities 
O(1 + lg σ
lg lg n ) instead of O(lg lg σ ).6 Such complexities are convenient for not so
large alphabets.

4.2 Positional Inverted Indexes

These indexes retrieve the positions of any word in a text. They may store the text
compressed up to the zero-order entropy of the word sequence s[1..n], which allows
direct access to any word. In addition they store the list of the positions where each
distinct word occurs. These lists can be compressed up to a second zero-order entropy
space [48], so the overall space is at least 2nH0(s). By regarding s as a sequence over
an alphabet [1..ν] (corresponding here to the vocabulary), Theorem 2 represents s
within nH0(s) + o(n)(H0(s) + 1) bits, which provides state-of-the-art compression
ratios. Variant (ii) supports constant-time access to any text word s[i], and access to
the j th entry of the list of any word a (s.selecta(j )) in time O(lg lg ν). These two time
complexities are exchanged in variant (i), or both can be made constant by spending
nH0(s) redundancy for any constant  > 0 (using Corollary 4). The length of the
inverted lists can be stored within O(ν lg n) bits (we also need at least this space to
store the sequence content of each word identiﬁer).

6One can retain lg lg n in the denominator by using block sizes depending on n and not on |si|, as explained
in the footnote at the beginning of Sect. 3.2.

244

Algorithmica (2014) 69:232–268

Apart from supporting this basic access to the list of each word, this representation 
easily supports operations that are more complex to implement on explicit
inverted lists [5]. For example, we can ﬁnd the phrases formed by two words w1
and w2, that appear n1 and n2 times, by ﬁnding the occurrences of one and verifying 
the other in the text, in time O(min(n1, n2) lg lg ν). Other more sophisticated
intersection algorithms [5] can be implemented by supporting operations such as
“ﬁnd the position in the list of w2 that follows the j th occurrence of word w1”
(s.rankw2 (s.selectw1 (j ))+ 1, in time O(lg lg ν)) or “give the list of word w restricted
to the range [x..y] in the collection” (s.selectw(s.rankw(x − 1) + j ), for j ≥ 1, until
exceeding y, in time O(lg lg ν) plus O(1) per retrieved occurrence). In Sect. 8.4 we
evaluate this representation in practice.

4.3 Binary Relations and Non-positional Inverted Indexes
Let R ⊆ L× O, where L = [1..λ] are called labels and O = [1..κ] are called objects,
be a binary relation consisting of n pairs. Barbay et al. [3] represent the relation
as follows. Let li1 < li2 < ··· < lik be the labels related to an object o ∈ O. Then
we deﬁne sequence so = li1 li2
··· lik . The representation for R is the concatenated
sequence s = s1 · s2 ··· sκ , of length n, and the bitmap b = 10
length n + κ + 1.
This representation allows one to efﬁciently support various queries [3]:
table_access: is l related to o?, s.rankl(b.rank0(b.select1(o + 1))) >

|s2| ··· 10

1, of

|s1|

10

|sκ|

object_select: the ith label related to an object o,
object_nb: the number of labels an object o is related to, b.select1(o + 1) −

s.rankl(b.rank0(b.select1(o)));
s.access(b.rank0(b.select1(o) + i));
b.select1(o) − 1;
object_rank: the number of labels < l an object o is related to, carried out with
a predecessor search in s[b.rank0(b.select1(o))..b.rank0(b.select1(o + 1))], an
area of length O(λ). The predecessor data structure requires o(n) bits as it is
built over values sampled every lg2 λ positions, and the query is completed with
a binary search;

label_select: the ith object related to a label l, b.rank1(b.select0(s.selectl(i)));
label_nb: the number of objects a label l is related to, s.rankl(n). It can also be
solved like object_nb, using a bitmap similar to b that traverses the table
label-wise;

label_rank: the number of objects <o a label l is related to,

s.rankl(b.rank0(b.select1(o))).
Bitmap b can be represented within O(κ lg n

κ ) = o(n) + O(κ) bits and support all
the operations in constant time [55], and its label-wise variant needs o(n) + O(λ)
bits. The rest of the space and time complexities depend on how we represent s.
Barbay et al. [3] used Golynski et al.’s representation for s [28], so they achieved
n lg λ + o(n lg λ) bits of space, and the times at rows 2 or 3 in Table 1 for the operations 
on s (later, Barbay et al. [4] achieved nHk(s) + o(n lg λ) bits and slightly
worse times). By instead representing s using Theorem 2, we achieve compressed
redundancy and slightly improve the times.

Algorithmica (2014) 69:232–268

245

To summarize, we achieve nH0(s)+ o(n)(H0(s)+ 1)+ O(κ + λ) bits, and solve
label_nb and object_nb in constant time, and table_access and label_rank 
in time O(lg lg λ). For label_select, object_select and object_rank 
we achieve times O(1), O(lg lg λ) and O((lg lg λ)2), respectively, or
O(lg lg λ), O(1) and O(lg lg λ), respectively. Corollary 4 yields a slightly larger
representation with improved times, and a multiary wavelet tree [29, Theorem 4]
achieves less space and different times; we leave the details to the reader.

(cid:9)

v nv lg n
nv

A non-positional inverted index is a binary relation that associates each vocabulary 
word with the documents where it appears. A typical representation of the
lists encodes the differences between consecutive values, achieving overall space
O(
), where word v appears in nv documents [61]. In our representation as
a binary relation, it turns out that H0(s) = (cid:9)
, and thus the space achieved is
comparable to the classical schemes. Within this space, however, the representation
offers various interesting operations apart from accessing the ith element of a list
(using label_select), including support for various list intersection algorithms;
see Barbay et al. [3, 4] for more details.

v nv lg n
nv

5 Compressing Permutations

Barbay and Navarro [6] measured the compressibility of a permutation π in terms
of the entropy of the distribution of the lengths of runs of different kinds. Let π be
covered by ρ runs (using any of the previous deﬁnitions of runs [6, 40, 44]) of lengths
runs(π ) = (cid:10)n1, . . . , nρ(cid:11). Then H(runs(π )) = (cid:9)
≤ lg ρ is called the entropy
of the runs (and, because ni ≥ 1, it also holds nH(runs(π )) ≥ (ρ − 1) lg n). In their
most recent variant [7] they were able to store π in 2nH(runs(π ))+ o(n)+O(ρ lg n)
bits for runs consisting of interleaved sequences of increasing or decreasing values,
and nH(runs(π )) + o(n) + O(ρ lg n) bits for contiguous sequences of increasing or
decreasing values (or, alternatively, interleaved sequences of consecutive values). In
lg ρ
lg lg n ) time, which on average drops
all cases they can compute π() and π
to O(1 + H(runs(π ))

) if the queries are uniformly distributed in [1..n].

−1() in O(

ni
n lg n
ni

lg lg n

We now show how to use access/rank/select data structures to support the operations 
more efﬁciently while retaining compressed redundancy space. In general terms,
we exchange their O(ρ lg n) space term by o(n)H(runs(π )), and improve their times
to O(lg lg ρ) in the worst case, and to O(lgH(runs(π ))) on average (again, this is an
improvement only if ρ is not too small).

We ﬁrst consider interleaved sequences of increasing or decreasing values as ﬁrst
deﬁned by Levcopoulos and Petersson [40] for adaptive sorting, and later on for compression 
[6], and then give improved results for more restricted classes of runs. In
both cases we ﬁrst consider the application of the permutation π() and its inverse,
−1(), and later show how to extend the support to the iterated application of the
π
permutation, π k(), extending and improving previous results [47].

Theorem 6 Let π be a permutation on n elements that consists of ρ interleaved
increasing or decreasing runs, of lengths runs(π ). Suppose we have a data structure
that stores a sequence s[1..n] over effective alphabet [1..ρ] within ψ(n, ρ,H0(s))

246

Algorithmica (2014) 69:232–268

−1() queries in time O(τ (n, ρ)).

bits, supporting queries access, rank, and select in time τ (n, ρ). Then, given its run
decomposition, we can store π in 2ψ(n, ρ,H(runs(π ))) + ρ bits, and perform π()
and π
Proof We ﬁrst replace all the elements of the rth run by r, for 1 ≤ r ≤ ρ. Let s be
(cid:4)[π(i)] =
the resulting sequence and let s
s[i]. We store s and s
using the given sequence representation, and also store ρ bits
) =
indicating whether each run is increasing or decreasing. Note that H0(s) = H0(s
H(runs(π )), which gives the claimed space.

be s permuted according to π, that is, s

(cid:4)

(cid:4)

(cid:4)

Notice that an increasing run preserves the relative order of the elements of a
.ranks[i](π(i)) =

subsequence. Therefore, if π(i) is part of an increasing run, then s
s.ranks[i](i), so

(cid:4)

π(i) = s

(cid:4)

.selects[i]

(cid:2)
(cid:3)
s.ranks[i](i)

.

If, instead, π(i) is part of a decreasing run, then s
1 − s.ranks[i](i), so

π(i) = s

(cid:4)

.selects[i]

(cid:2)
(cid:3)
s.ranks[i](n) + 1 − s.ranks[i](i)

.

(cid:4)

.ranks[i](π(i)) = s.ranks[i](n) +

(cid:4)

−1() query is symmetric (exchange s and s

−1 with O(1) calls to access, rank, and select on s or s

in the formulas). Therefore we comA 
π
(cid:2)
pute π() and π
Example 3 Let π = 1, 8, 9, 3, 6, 10, 5, 4, 11, 7, 2, 12 be formed by three runs (indi-
(cid:4) = (1, 2, 1, 2, 1,
cated by the different fonts). Then s = (1, 2, 3, 1, 2, 3, 1, 2, 3) and s
2, 1, 2, 3, 3, 3, 3).

(cid:4)

.

By combining Theorem 6 with the representations in Theorem 2, we obtain a result
that improves upon previous work [6, 7] in time complexity. Note that if the queried
positions i are uniformly distributed in [1..n], then all the access, rank, and select
queries follow the same character distribution of the runs, and Corollary 3 applies.
Note also that the ρ bits are contained in o(n)H(runs(π )) because nH(runs(π )) ≥
(ρ − 1) lg n.

Corollary 7 Let π be a permutation on n elements that consists of ρ interleaved
increasing or decreasing runs, of lengths runs(π ). Then, given its run decomposition,
we can store π in 2nH(runs(π )) + o(n)(H(runs(π )) + 1) bits and perform π() and
−1() queries in O(lg lg ρ) time. On uniformly distributed queries the average times
are O(lgH(runs(π ))).

π

The case where the runs are contiguous is handled within around half the space,

as a simpliﬁcation of Theorem 6.

Corollary 8 Let π be a permutation on n elements that consists of ρ contiguous 
increasing or decreasing runs, of lengths runs(π ). Suppose we have a
data structure that stores a sequence s[1..n] over effective alphabet [1..ρ] within
ψ(n, ρ,H0(s)) bits, supporting queries access and rank in time τar(n, ρ), and

Algorithmica (2014) 69:232–268

247

ρ

−1() queries in time O(τar(n, ρ)).

select in time τs(n, ρ). Then, given its run decomposition, we can store π in
ψ(n, ρ,H(runs(π ))) + ρ lg n
+ O(ρ)+ o(n) bits of space, and perform π() queries
in time O(τs(n, ρ)) and π
Proof We proceed as in Theorem 6, yet now sequence s is of the form s =
1n12n2 ··· ρnρ , and therefore it can be represented as a bitmap b = 10n1−110n2−1 ···
10nρ−11. The required operations are implemented as follows: s.access(i) =
b.rank1(i), s.ranks[i](i) = i− b.select1(s[i])+1, s.ranks[i](n) = b.select1(s[i]+1)−
b.select1(s[i]), and s.selecta(i) = b.select1(a) + i − 1. Those operations are solved
in constant time using a representation for b that takes (ρ + 1) lg(e(n+ 1)/(ρ + 1))+
o(n) bits [55]. Added to the ρ bits that mark increasing or decreasing sequences, this
gives the claimed space. The claimed time complexities correspond to the operations
(cid:2)
on s

, as those in s take constant time.

(cid:4)

Once again, by combining the corollary with representation (i) in Theorem 2,
we obtain results that improve upon previous work [6, 7]. The ρ lg n
ρ bits are in
o(n)(H(runs(π )) + 1) because they are o(n) as long as ρ = o(n), and otherwise
they are O(ρ) = o(ρ lg n), and (ρ − 1) lg n ≤ nH(runs(π )).

Corollary 9 Let π be a permutation on n elements that consists of ρ contiguous
increasing or decreasing runs, of lengths runs(π ). Then, given its run decomposition,
we can store π in nH(runs(π ))+ o(n)(H(runs(π ))+1) bits and perform π() queries
−1() queries in time O(lg lg ρ) (and O(lgH(runs(π ))) on average
in time O(1) and π
for uniformly distributed queries).

If π is formed by interleaved but strictly incrementing (+1) or decrementing (−1)
−1 is formed by contiguous runs, in the same number and length [6]. This

runs, then π
gives an immediate consequence of Corollary 8.

Corollary 10 Let π be a permutation on n elements that consists of ρ interleaved 
strict increasing or decreasing runs, of lengths runs(π ). Suppose we have
a data structure that stores a sequence s[1..n] over effective alphabet [1..ρ] within
ψ(n, ρ,H0(s)) bits, supporting queries access and rank in time τar(n, ρ), and
select in time τs(n, ρ). Then, given its run decomposition, we can store π in
ψ(n, ρ,H(runs(π ))) + ρ lg n
+ O(ρ)+ o(n) bits of space, and perform π() queries
in time O(τar(n, ρ)) and π

−1() queries in time O(τs(n, ρ)).

ρ

For example we can achieve the same space of Corollary 9, yet with the times for
−1 reversed. Finally, if we consider runs for π that are both contiguous and
−1. Corollary 8 can be further

π and π
incrementing or decrementing, then so are the runs of π
simpliﬁed as both s and s

can be represented with bitmaps.

(cid:4)

Corollary 11 Let π be a permutation on n elements that consists of ρ contiguous
and strict increasing or decreasing runs, of lengths runs(π ). Then, given its run de-
+ O(ρ) + o(n) bits, and perform π() and
composition, we can store π in 2ρ lg n
−1() in O(1) time.
ρ
π

248

Algorithmica (2014) 69:232–268

We now show how to achieve exponentiation, π k(i) or π

−k(i), within compressed
space. Munro et al. [47] reduced the problem of supporting exponentiation on a permutation 
π to the support of the direct and inverse application of another permutation,
related but with quite distinct runs than π. Combining it with any of our results does
yield compression, but one where the space depends on the lengths of both the runs
and cycles of π. The following construction, extending the technique by Munro et
al. [47], retains the compressibility in terms of the runs of π, which is more natural.
It builds an index that uses small additional space to support the exponentiation, thus
allowing the compression of the main data structure with any of our results.

Theorem 12 Suppose we have a representation of a permutation π on n elements
. Then for
that supports queries π() in time τ
any t ≤ n, we can build a data structure that takes O((n/t ) lg n) bits and, used
−k() queries in
in conjunction with operation π() or π
O(t min(τ

and queries π
−1(), supports π k() and π

−1() in time τ

)) time.

+

−

+

−

, τ

(cid:4)

(cid:4)

(cid:4)

(cid:4)

(cid:4)−j (i) or π j

, and ﬁnally ﬁnd the position i

(cid:4) = (j + k mod (cid:7)), hence π k(i) = π j

(cid:4) = π k(i) is to discover that i is in a cycle of length
Proof The key to computing i
(cid:7) and to assign it a position 0 ≤ j < (cid:7) within its cycle (note j is arbitrary, yet we
must operate consistently once it is assigned). Then π k(i) lies in the same cycle, at
(cid:4)+(cid:7)−j (i). Thus all we need
position j
in π that corresponds
is to ﬁnd out j and (cid:7), compute j
(cid:4)
th element of the cycle. We decompose π into its cycles and, for every cycle
to the j
of length at least t, store the cycle’s length (cid:7) and an array containing the position
i in π of every tth element in the cycle. Those positions i are called ‘marked’. We
also store a binary sequence b[1..n], so that b[i] = 1 iff i is marked. For each marked
element i we record to which cycle i belongs and the position j of i in its cycle. To
compute π k(i), we repeatedly apply π() at most t times until we either loop or ﬁnd a
marked element. In the ﬁrst case, we have found (cid:7), so we can assume j = 0, compute
< (cid:7) ≤ t, and apply π() at most t more times to ﬁnd i
(i) = π k(i) in the loop.
(cid:4)
j
If we reach a marked element, instead, we have stored the cycle identiﬁer to which i
and know that the previous marked
belongs, as well as j and (cid:7). Then we compute j
/t of the
position is j
by applying
array of positions of marked elements, and we ﬁnally move from i
(cid:4)− j
−k query is similar (note
j
−1() as we can always move forward). Moreover, we can
that it does not need to use π
−1() instead of π(), whichever is faster, to compute both π k()
also proceed using π
−k(). The space is O((n/t ) lg n) both for the samples and for a compressed
and π
representation of bitmap b. Note that we only compute rank at the positions i such
that b[i] = 1. Thus we can use the ID structure [55], which uses O((n/t ) lg t ) bits. (cid:2)

/t(cid:14). The corresponding position i

∗ ≤ t times operation π(), i = π j

) = π k(i). A π

is found at cell j

∗ = t · (cid:13)j

∗

(cid:4)−j

(cid:4) = π j

(cid:4)

∗

(i

∗

∗

∗

(cid:4)

to i

5.1 Application to Self-Indexes

These results on permutations apply to a second family of self-indexes, which is
based on the representation of the so-called Ψ function [32, 35, 57]. Given the sufﬁx 
array A[1..n] of sequence s[1..n] over alphabet [1..σ], Ψ is deﬁned as Ψ (i) =

Algorithmica (2014) 69:232–268

249

−1[(A[i] mod n)+ 1]. Counting, locating, and extracting is carried out through perA

mutation Ψ , which replaces s and A. It is known [35] that Ψ contains σ contiguous 
increasing runs so that H(runs(Ψ )) = H0(s), which allows for its compression.
Grossi et al. [32] represented Ψ within nHk(s) + O(n) bits, while supporting operation 
Ψ () in constant time, or within nHk(s) + o(n lg σ ) while supporting Ψ ()
in time O(lg σ ). By using Corollary 9, we can achieve the unprecedented space
nH0(s) + o(n)(H0(s) + 1) and support Ψ () in constant time. In addition we can
−1() allows for
support the inverse Ψ
bidirectional indexes [56], which can for example display a snippet around any occurrence 
found without the need for any extra space for sampling. Our construction
of Theorem 12 can be applied on top of any of those representations so as to support
operation Ψ k(), which is useful for example to implement compressed sufﬁx trees,
yet the particularities of Ψ allow for sublogarithmic-time solutions [32]. Note also
that using Huffman-shaped wavelet trees to represent the permutation [7] yields even
less space, nH0(s)+ o(n)+O(σ lg n) bits, and the time complexities are relevant for
not so large alphabets.

−1() in time O(lg lg σ ). Having both Ψ () and Ψ

6 Compressing Functions
Hreinsson, Krøyer and Pagh [38] recently showed how, given a domain X =
{x1, x2, . . . , xn} ⊂ N of numbers that ﬁt in a machine word, they can represent any
f : X → [1..σ] in compressed form and provide constant-time evaluation. Let us
identify function f with the sequence of values f[1..n] = f (x1)f (x2)··· f (xn).
Then their representation uses at most (1 + )nH0(f ) + O(n) + o(σ ) bits, for any
constant  > 0. We note that this bound holds even when σ is much larger than n.
In the special case where X = [1..n] and σ = o(n), we can achieve constant-time
evaluation and a better space bound using our sequence representations. Moreover,
we can support extra functionality such as computing the pre-image of an element.
A ﬁrst simple result is obtained by representing f as a sequence.
Lemma 13 Let f : [1..n] → [1..σ] be a function. We can represent f using
nH0(f )+ o(n)(H0(f )+ 1)+O(σ ) bits and compute f (i) for any i ∈ [1..n] in O(1)
−1(a) for any a ∈ [1..σ] in time O(lg lg σ ), or vice versa.
time, and any element of f
Using more space, (1+ )H0(f )+ o(n) bits for any constant  > 0, we support both
queries in constant time. The size |f
Proof We represent sequence f[1..n] using Theorem 2 or Corollary 4, so f (i) =
−1(a)| in
f.access(i) and the j th element of f
constant time we store a binary sequence b = 10
−1(σ )|
1, so
that |f
−1(a)| = b.select1(a + 1) − b.select1(a) − 1. The space is the one needed
to represent s plus O(σ lg n
σ ) bits to represent b using an ID [55]. This is o(n) if
σ = o(n), and otherwise it is O(σ ). This extra space is also necessary because [1..σ]
may not be the effective alphabet of sequence f[1..n] (if f is not surjective).
(cid:2)

−1(a) is f.selecta(j ). To compute |f
1··· 10
|f

−1(a)| is always computed in O(1) time.

|f

−1(2)|

|f

−1(1)|

10

Another source of compressibility frequently arising in real-life functions is nondecreasing 
or nonincreasing runs. Let us start by allowing interleaved runs. Note that

250

Algorithmica (2014) 69:232–268

in this case H(runs(f )) ≤ H0(f ), where equality is achieved if we form runs of equal
values only.
Theorem 14 Let f : [1..n] → [1..σ] be a function such that sequence f[1..n] consists 
of ρ interleaved non-increasing or non-decreasing runs. Then, given its run decomposition,
 we can represent f in 2nH(runs(f )) + o(n)(H(runs(f )) + 1) + O(σ )
bits and compute f (i) for any i ∈ [1..n], and any element in f
−1(a) for any
a ∈ [1..σ], in time O(lg lg ρ). The size |f

−1(a)| is computed in O(1) time.

Proof We store function f as a combination of the permutation π that stably sorts
the values f (i), plus the binary sequence b of Lemma 13. Therefore, it holds

Similarly, the j th element of f
(cid:2)

(cid:2)
b.select0

f (i) = b.rank1
−1(a) is
(cid:2)
b.select1(a)

b.rank0

π

(cid:2)
π

(cid:3)(cid:3)
−1(i)
.

(cid:3) + j

(cid:3)
.

−1 has the same runs as f (the runs in f can have equal values but those of
Since π
−1 using Corollary 7 to obtain the claimed time and
−1 cannot), we can represent π
π
(cid:2)
space complexities.
Example 4 Let f[1..9] = π
−1(1, 3, 2, 5, 4, 9, 8, 9, 8). The odd positions form an
increasing run (1, 2, 4, 8, 8) and the even positions form (3, 5, 9, 9). The permu-
−1 =
tation π sorting the values is (1, 3, 2, 5, 4, 7, 9, 6, 8), and its inverse is π
(1, 3, 2, 5, 4, 8, 6, 9, 7). The bitmap b is 101010101011100100.

If we consider only contiguous runs in f , we obtain the following result by repre-
−1 with Corollary 9. Note the entropy of contiguous runs is no longer upper

senting π
bounded by H0(f ).
Corollary 15 Let f : [1..n] → [1..σ] be a function, where sequence f consists of ρ
contiguous non-increasing or non-decreasing runs. Then, given its run decomposition,
 we can represent f in nH(runs(f )) + o(n)(H(runs(f )) + 1) + O(σ ) bits, and
−1(a) in time
compute any f (i) in O(1) time, as well as retrieve any element in f
O(lg lg ρ). The size |f

−1(a)| can be computed in O(1) time.

In all the above results we can use Huffman-shaped wavelet trees [7] to obtain an

alternative space/time tradeoff. We leave the details to the reader.

6.1 Application to Binary Relations, Revisited

Recall Sect. 4.3, where we represent a binary relation in terms of a sequence s and a
bitmap b. By instead representing s as a function, we can capture another source of
compressibility, and achieve slightly different time complexities. Note that H0(s) corresponds 
to the distribution of the number oi of objects associated with a label i, let
us call it Hlab = H0(s) = (cid:9)
. On the other hand, if we regard the contiguous

oi
n lg n
oi

Algorithmica (2014) 69:232–268

251

li
n lg n
li

increasing runs of s, the entropy corresponds to the distribution of the number li of
labels associated with an object i, let us call it Hobj = H(runs(s)) = (cid:9)
. While
Sect. 4.3 compresses B in terms of Hlab = H0(s), we can use Corollary 15 to achieve
nHobj + o(n)(Hobj + 1) + O(κ + λ) bits of space. Since f.access(i) = f (i) and
−1(a), this representation solves label _nb,
f.selecta(j ) is the j th element of f
object_nb and object _select in constant time, and label_select and
object_rank in time O(lg lg λ). Operations label_rank and table_access
require f.rank, which is not directly supported. The former can be solved in time
O(lg lg λ lg lg κ) as a predecessor search in π (storing absolute samples every lg2 κ
positions ), and the latter in time O(lg lg λ) as the difference between two object_rank 
queries. We can also achieve nHobj + o(n) + O(κ + λ) bits using
Huffman-shaped wavelet trees; we leave the details to the reader.

7 Compressing Dynamic Collections of Disjoint Sets

ni
n lg n
ni

Finally, we now give what is, to the best of our knowledge, the ﬁrst result about
storing a compressed collection of disjoint sets while supporting operations union and
ﬁnd [60]. The key point in the next theorem is that, as the sets in the collection C are
merged, our space bound shrinks with the zero-order entropy of the distribution of the
function s that assigns elements to sets in C. We deﬁne H(C) = (cid:9)
≤ lg|C|,
where ni are the sizes of the sets, which add up to n.
Theorem 16 Let C be a collection of disjoint sets whose union is [1..n]. For any
 > 0, we can store C in (1+ )nH(C)+O(|C| lg n)+ o(n) bits and perform any sequence 
of r union and ﬁnd operations in O(rα(n) + (1/)n lg lg n) total time, where
α(n) is the inverse Ackermann’s function.
Proof We ﬁrst use Theorem 2 to store the sequence s[1..n] in which s[i] is the
representative of the set containing i. We then store the representatives in a standard 
disjoint-set data structure D [60]. Since H0(s) = H(C), our data structures
take nH(C) + o(n)(H(C) + 1) + O(|C| lg n) bits. We can perform a query ﬁnd(i)
on C by performing D.ﬁnd(s[i]), and perform a union(i, j ) operation on C by performing 
D.union(D.ﬁnd(s[i]), D.ﬁnd(s[j])). As we only need access functionality
on s, we use a simple variant of Theorem 2. We support only rank and select on
the multiary wavelet tree that represents sequence t, and store the s(cid:7) subsequences
as plain arrays. The mapping m is of length |C|, so it can easily be represented in
plain form to support constant-time operations, within O(|C| lg n) bits. This yields
constant time access, and therefore the cost of the r union and ﬁnd operations is
O(rα(n)) [60]. For our data structure to shrink as we merge sets, we keep track of
H(C) and, whenever it shrinks by a factor of 1 + , we rebuild our entire data structure 
on the updated values s[i] ← ﬁnd(s[i]). First, note that all those ﬁnd operations
take O(n) time because of path-compression [60]: Only the ﬁrst time one accesses a
node v ∈ D it may occur that the representative is not directly v’s parent. Thus the
overall time can be split into O(n) time for the n instructions ﬁnd(s[i]) plus O(n) for
the n times a node v ∈ D is visited for the ﬁrst time. Reconstructing the structure of

252

Algorithmica (2014) 69:232–268

Theorem 2 also takes O(n) time. The plain structures for m and s(cid:7) are easily built
in linear time, and so is the multiary wavelet tree supporting rank and access [22],
as it requires just tables of sampled counters. Since H(C) is always less than lg n,
we rebuild only O(log1+ lg n) = O((1/) lg lg n) times. Thus the overall cost of rebuilding 
is O((1/)n lg lg n). This completes our time complexity. Finally, the space
term o(n)H(C) is absorbed by H(C) by slightly adjusting , and this gives our ﬁnal
(cid:2)
space formula.

8 Experimental Results

In this section we explore the performance of our structure in practice. We ﬁrst introduce,
 in Sect. 8.1, a simpler and more practical alphabet partitioning scheme we call
“dense”, which experimentally performs better than the one we describe in Sect. 3,
although in theory has an O(n)-bit redundancy term in the space. Next, in Sect. 8.2
we study the performance of both alphabet partitioning methods, as well as the optimal 
one [59], in terms of compression ratio and decompression performance. Given
the results of these experiments, we continue only with our dense partitioning for the
rest of the section. In Sect. 8.3 we compare our new sequence representation with the
state of the art, considering the tradeoff between space and time of operations rank,
select, and access. Then, Sects. 8.4, 8.5, and 8.6 compare the same data structures on
different real-life applications of sequence representations. In the ﬁrst, the operations
are used to emulate an inverted index on the compressed sequence using (almost) no
extra space. In the second, they are used to emulate self-indexes for text [48]. In the
third, they provide access to direct and reverse neighbors on graphs represented with
adjacency lists. The machine used for the experiments has an Intel® Xeon® E5620
at 2.40 GHz, 94 GB of RAM. We did not use multithreading in our implementations;
 times are measured using only one thread, and in RAM. The operating system
is Ubuntu 10.04, with kernel 2.6.32-33-server.x86_64. The code was compiled using 
GNU/GCC version 4.4.3 with optimization ﬂags -O9. Our code is available in
LIBCDS version 1.0.10, downloadable from http://libcds.recoded.cl/.

8.1 Dense Alphabet Partitioning

Said [59] proved that an optimal assignment to sub-alphabets must group consecutive
symbols once sorted by frequency. A simple alternative to the partitioning scheme
presented in Sect. 3, and that follows this optimality principle, is to make mapping m
group elements into consecutive chunks of doubling size, that is, m[a] = (cid:13)lg r(a)(cid:14),
where r(a) is the rank of a according to its frequency. The rest of the scheme to
deﬁne t[1..n] and the sequences s(cid:7)[1..σ(cid:7)] is as in Sect. 3. The classes are in the
range 0 ≤ (cid:7) ≤ (cid:13)lg σ(cid:14), and each element in s(cid:7) is encoded in (cid:7) bits. As we use all
the available bits of each symbol in sequences s(cid:7) (except possibly in the last one),
we call this scheme dense. We show that this scheme is not much worse than the
one proposed in Sect. 3 (which will be called sparse). First consider the total num-
|s|a(cid:13)lg r(a)(cid:14). Since
ber of bits we use to encode the sequences s(cid:7),
|s|a ≤ n/r(a) because the symbols are sorted by decreasing frequency, it holds that

(cid:7) (cid:7)|s(cid:7)| = (cid:9)

(cid:9)

a

Algorithmica (2014) 69:232–268

253

(cid:9)|s|a(cid:13)lg r(a)(cid:14) ≤ (cid:9)|s|a lg(n/|s|a) = nH0(s). Now consider the
r(a) ≤ n/|s|a and
number of bits we use to encode t = (cid:13)lg r(s[1])(cid:14), . . . ,(cid:13)lg r(s[n])(cid:14). We could store
each element (cid:13)lg r(s[i])(cid:14) of t
in 2(cid:13)lg((cid:13)lg r(s[i])(cid:14) + 1)(cid:14) − 1 bits using γ -codes
[61], and such encoding would be lower bounded by nH0(t ). Thus nH0(t ) ≤ 2
(cid:9)
(cid:9)
|s|a lg lg(n/|s|a + 1) = O(n(lgH0(s) + 1)) =
i lg lg(r(s[i]) + 1) ≤ 2
o(nH0(s)) + O(n) (recall Sect. 3.2). It follows that the total encoding length is
a
nH0(s) + O(n lgH0(s)) = nH0(s) + o(nH0(s)) + O(n) bits. Apart from the pretty
tight upper bound, it is not evident whether this scheme is more or less efﬁcient than
the sparse encoding. Certainly the dense scheme uses the least possible number of
classes (which could allow storing t in plain form using lg lg σ bits per symbol). On
the other hand, the sparse method uses in general more classes, which allows for
smaller sub-alphabets using fewer bits per symbol in sequences s(cid:7). As we will see in
Sect. 8.2, the dense scheme uses less space than the sparse one for t, but more for the
sequences s(cid:7).
Example 5 Consider the same sequence s = "alabar a la alabarda" of
Example 1. The dense partitioning will assign m[a] = 0, m[l] = m["] = 1,
m[b] = m[r] = m[d] = 2. So the sequence of sub-alphabet identiﬁers is t[1..20] =
(0, 1, 0, 2, 0, 2, 1, 0, 1, 1, 0, 1, 0, 1, 0, 2, 0, 2, 2, 0), and the subsequences are
s0 = (1, 1, 1, 1, 1, 1, 1, 1, 1), s1 = (2, 1, 1, 2, 1, 2), and s2 = (1, 3, 1, 3, 2). This
dense scheme uses 16 bits for the sequences s(cid:7), and the zero-order compressed t
requires nH0(t ) = 30.79 bits. The overall compression is 2.34 bits per symbol. The
sparse partitioning of Example 1 used 10 bits in the sequences s(cid:7), and the zero-order
compressed t required nH0(t ) = 34.40 bits. The total gives 2.22 bits per symbol. In
our real applications, the dense partitioning performs better.

Note that the question of space optimality is elusive in this scenario. Since the
encoding in t plus that in the corresponding sequence s(cid:7) forms a unique code per
symbol, the optimum is reached when we choose one sub-alphabet per symbol, so
that the sequences s(cid:7) require zero bits and all the space is in nH0(t ) = nH0(s). The
alphabet partitioning always gives away some space, in exchange for faster decompression 
(or, in our case, faster rank/select/access operations). Said’s optimal partitioning 
[59] takes care of this problem by using a parameter k that is the maximum
number of sub-alphabets to use. We sort the alphabet by decreasing frequency and
call S(c, k) the total number of bits required to encode the symbols [c..σ] of the alphabet 
using a partitioning into at most k sub-alphabets. In general, we can make a
sub-alphabet with the symbols [c..c
(cid:4)] and solve optimally the rest, but if k = 1 we
(cid:4) = σ . When we can choose, the optimization formula is as
are forced to choose c
follows:
(cid:10)

-th in s. The ﬁrst term of the
where f is the total frequency of symbols c-th to c
sum accounts for the increase in tH0(s), the second for the size in bits of the
new sequence s(cid:7), and the third for the smaller subproblem, where it also holds
S(σ + 1, k) = 0 for any k. This dynamic programming algorithm requires O(kσ )
space and O(σ 2) time. We call this partitioning method optimal.

S(c, k) = min
c≤c
(cid:4)≤σ

f lg

n

f

+ f

(cid:4)

(cid:2)
lg
c

(cid:4) − c + 1

(cid:2)
c

(cid:4) + 1, k − 1

(cid:3)(cid:11)

,

(cid:3)(cid:5) + S
(cid:4)

254

Algorithmica (2014) 69:232–268

Example 6 The optimal partitioning using 3 classes just like the dense approach
in Example 5 leaves ’a’ in its own class, then groups ’l’, ", ’b’ and ’r’ in
a second class, and ﬁnally leaves ’d’ alone in a third class. The overall space

nH0(t ) + (cid:9)|s(cid:7)|(cid:5)lg σ(cid:7)(cid:6) is 2.23 bits per symbol, less than the 2.34 reached by the

dense partitioning. If, instead, we let it use four classes, it gives the same solution as
the sparse method in Example 1.

Finally, in Sect. 3 we represent the sequences s(cid:7) with small alphabets σ(cid:7) using
wavelet trees (just like t) instead of using the representation of Golynski et al. [28],
which is used for large σ(cid:7) > lg n. In theory, this is because Golynski et al.’s representation 
does not ensure sublinearity on smaller alphabets when used inside our
scheme. While this may appear to be a theoretical issue, the implementation of such
data structure (e.g., in LIBCDS) is indeed unattractive for small alphabets. For this
reason, we also avoid using it on the chunks where σ(cid:7) is small (in our case, the ﬁrst
ones). Note that using a wavelet tree for t and then another for the symbols in a sequence 
s(cid:7) is equivalent to replacing the wavelet tree leaf corresponding to (cid:7) in t by
the whole wavelet tree of s(cid:7). The space used by such an arrangement is worse than
the one obtained by building, from scratch, a wavelet tree for t where the symbols
t[i] = (cid:7) are actually replaced by the corresponding symbol s[i].

In our dense representation we use a parameter (cid:7)min that controls the minimum (cid:7)
value that is represented outside of t. All the symbols that would belong to s(cid:7), for
(cid:7) < (cid:7)min, are represented directly in t. Note that, by default, since σ0 = 1, we have
(cid:7)min = 1.

8.2 Compression Performance

For all the experiments in Sect. 8, except Sect. 8.6, we used real datasets extracted
from Wikipedia. We considered two large collections, Simple English and Spanish,
dated from 06/06/2011 and 03/02/2010, respectively. Both are regarded as sequences
of words, not characters. These collections contain several versions of each article.
Simple English, in addition, uses a reduced vocabulary. We collected a sample of
100,000 versions at random from all the documents of Simple English, which makes
a long and repetitive sequence over a small alphabet. For the Spanish collection,
which features a much richer vocabulary, we took the oldest version of each article,
which yields a sequence of similar size, but with a much larger alphabet.

We generated a single sequence containing the word identiﬁers of all the articles
concatenated, obtained after stemming the collections using Porter for English and
Snowball for Spanish. Table 2 shows some basic characteristics of the sequences
obtained.7

We measured the compression ratio achieved by the three partitioning schemes,
dense, sparse, and optimal. For dense we did not include any individual
symbols (other than the most frequent) in sequence t, i.e., we let (cid:7)min = 1. For optimal 
we allow 1 + (cid:13)lg σ(cid:14) sub-alphabets, just like dense.

7The code for generating these sequences is available at https://github.com/fclaude/txtinvlists.

Algorithmica (2014) 69:232–268

Table 2 Main characteristics of the datasets used

Collection

Articles

Total words (n)

Distinct words (σ )

255

Entropy (H0(s))

Simple English
Spanish

100,000
1,590,453

766,968,140
511,173,618

664,194
3,210,671

11.60
11.37

Fig. 1 Space versus decompression time for basic and alphabet-partitioned schemes. The vertical line
marks the zero-order entropy of the sequences. AC can slightly break the entropy barrier on Simple English
because it is adaptive

In all cases, the symbols in each s(cid:7) are represented using (cid:5)lg σ(cid:7)(cid:6) bits. The sequence
of classes t, instead, is represented in three different forms: Plain uses a ﬁxed number 
of bits per symbol, (cid:5)lg (cid:7)(cid:6) where (cid:7) is the maximum class; Huff uses Huffman
coding of the symbols,8 and AC uses Arithmetic coding of the symbols.9 The former
encodings are faster, whereas the latter use less space. In addition we consider compressing 
the original sequences using Huffman (Huffman) and Arithmetic coding
(Arith).

As explained, the main interest in using alphabet partitioning in a compressor is
to speed up decompression without sacriﬁcing too much space. Figure 1 compares
all these alternatives in terms of space usage (percentage of the original sequence)
and decompression time per symbol. It can be seen that alphabet partitioning combined 
with AC compression of t wastes almost no space due to the partitioning,
and speeds up considerably the decompression of the bare AC compression. However,
 bare Huffman also uses the same space and decompresses several times faster.
Therefore, alphabet partitioning combined with AC compression is not really interesting.
 The other extreme is the combination with a Plain encoding of t. In the
best combinations, this alphabet partitioning wastes close to 10 % of space, and in
exchange decompresses around 30 % faster than bare Huffman. The intermediate

8We use G. Navarro’s Huffman implementation; the code is available in LIBCDS.
9We use the code by J. Carpinelli, A. Moffat, R. Neal, W. Salamonsen, L. Stuiver, A. Turpin and I. Witten,
available at http://ww2.cs.mu.oz.au/~alistair/arith_coder/arith_coder-3.tar.gz. We modiﬁed the decompressor 
to read the whole stream before timing decompression.

256

Table 3 Breakdown, in bits per
symbol, of the space used in
sequence t versus the space used
in all the sequences s(cid:7), for the
different combinations. The
third column in each collection
is the percentage of symbols that
lie in sequences s(cid:7) with alphabet
sizes σ(cid:7) = 1

Algorithmica (2014) 69:232–268

Combination

Simple English

t

s(cid:7)

%

Spanish

t

s(cid:7)

Plain-dense
Plain-sparse
Plain-optimal
Huff-dense
Huff-sparse
Huff-optimal
AC-dense
AC-sparse
AC-optimal

4.96
9.97
4.96
3.99
8.22
4.08
3.95
8.18
4.03

7.74
3.79
7.66
7.74
3.79
7.66
7.74
3.79
7.66

6.67
19.01
10.21
6.67
19.01
10.21
6.67
19.01
10.21

4.76
9.79
4.76
4.13
8.01
4.24
4.10
7.99
4.20

7.72
4.08
7.61
7.72
4.08
7.61
7.72
4.08
7.61

%

11.70
32.91
16.88
11.70
32.91
16.88
11.70
32.91
16.88

combination, Huff, wastes less than 1 % of space, while improving decompression
time by almost 25 % over bare Huffman.

Another interesting comparison is that of partitioning methods. In all cases, variant
dense performs better than sparse. The difference is larger when combined with
Plain, where sparse is penalized for the larger alphabet size of t, but still there
is a small difference when combined with AC, which shows that t has also (slightly)
lower entropy in variant dense. Table 3 gives a breakdown of the bits per symbol in
t versus the sequences s(cid:7) in all the methods. It can be seen that sparse leaves much
more information on sequence t than the alternatives, which makes it less appealing
since the operation of t is slower than that of the other sequences. However, this can
be counterweighted by the fact that sparse produces many more sequences with
alphabet size 1, which need no time for accessing. It is also conﬁrmed that dense
leaves slightly less information on t than optimal, and that the difference in space
between the three alternatives is almost negligible (unless we use Plain to encode t,
which is not interesting).

Finally, let us consider how the partitioning method affects decompression time,
given an encoding method for t. For method AC, sparse is signiﬁcantly slower.
This is explained by the t component having many more bits, and the decompression
time being dominated by the processing of t by the (very slow) arithmetic decoder.
For method Plain, instead, sparse is slightly faster, despite the fact that it uses
more space. Since now the reads on t and s(cid:7) take about the same time, this difference
is attributable to the fact that sparse leaves more symbols on sequences s(cid:7) with
alphabets of size 1, where only one read in t is needed to decode the symbol (see
Table 3). For Huff all the times are very similar, and very close to the fastest one.
Therefore, for the rest of the experiments we use the variant Huff with dense
partitioning, which performs best in space/time.

8.3 Rank, Select and Access

We now consider the efﬁciency in the support for the operations rank, select, and
access. We compare our sequence representation with the state of the art, as implemented 
in LIBCDS v1.0.10, a library of highly optimized implementations of comAlgorithmica 
(2014) 69:232–268

257

pact data structures. As said, LIBCDS already includes the implementation of our new
structure.

bitmap

WaveletTree) with

We compare six data structures for representing sequences. Those based on
trees are obtained in LIBCDS by combining sequence representations
wavelet
(WaveletTreeNoptrs,
representations
(BitSequenceRG, BitSequenceRRR) for the data on wavelet tree nodes.
• WTNPRG: Wavelet tree without pointers, obtained as WaveletTreeNoptrs+
BitSequenceRG in LIBCDS. This corresponds to the basic balanced wavelet tree
structure [32], where all the bitmaps of a level are concatenated [41]. The bitmaps
are represented in plain form and their operations are implemented using a onelevel 
directory [30] (where rank is implemented in time proportional to a sampling
step and select uses a binary search on rank). The space is n lg σ + o(n lg σ ) and
the times are O(lg σ ). In practice the absence of pointers yields a larger number of
operations to navigate in the wavelet tree, and also select operation on bitmaps is
much costlier than rank. A space/time tradeoff is obtained by varying the sampling
step of the bitmap rank directories.
• WTNPRRR: Wavelet tree without pointers with bitmap compression, obtained
in LIBCDS as WaveletTreeNoptrs+BitSequenceRRR. This is similar to
WTNPRG, but the bitmaps are represented in compressed form using the FID technique 
[55] (select is also implemented with binary search on rank). The space is
nH0(s) + o(n lg σ ) and the times are O(lg σ ). In practice the FID representation
makes it considerably slower than the version with plain bitmaps, yet select operation 
is less affected. A space/time tradeoff is obtained by varying the sampling step
of the bitmap rank directories.
• GMR: The representation proposed by Golysnki et al. [28], named SequenceGMR
in LIBCDS. The space is n lg σ + o(n lg σ ), yet the lower-order term is sublinear
on σ , not n. The time is O(1) for select and O(lg lg σ ) for rank and access, although 
on average rank is constant-time. A space/time tradeoff, which in practice
affects only the time for access, is obtained by varying the permutation sampling
inside the chunks [28].
• WTRG: Wavelet tree with pointers and Huffman shape, obtained as WaveletTree
+BitSequenceRG in LIBCDS. The space is nH0(s) + O(n) + o(nH0(s)) +
O(σ lg n). The time is O(lg σ ), but in our experiments it will be O(H0(s)) for
access, since the positions are chosen at random from the sequence and then we
navigate less frequently to deeper Huffman leaves.
tree with pointers, obtained with WaveletTree+
• WTRRR: Wavelet
BitSequenceRRR in LIBCDS. The space is nH0(s) + o(nH0(s)) + O(σ lg n).
The time is as in the previous structure, except that in practice the FID representation 
is considerably slower.
• AP: Our new alphabet partitioned structure, named SequenceAlphPart in
LIBCDS. We use dense partitioning and include the 210 most frequent symbols
directly in t, (cid:7)min = 10. Sequence t is represented with a WTRG (since its alphabet
is small and the pointers pose no signiﬁcant overhead), and the sequences σ(cid:7) are
represented with structures GMR. The space is nH0(s) + o(nH0(s)), although the
lower-order term is actually sublinear on σ (and only very slightly on n). The times
are as in GMR, although there is a small additive overhead due to the wavelet tree

258

Algorithmica (2014) 69:232–268

Fig. 2 Time for the three operations. The x axis starts at the entropy of the sequence

on t. A space/time tradeoff is obtained with the permutations sampling, just as in
GMR.

Figure 2 shows the results obtained for both text collections, giving the average
over 100,000 measures. The rank queries were generated by choosing a symbol from
[1..σ] and a position from [1..n], both uniformly at random. For select we chose the
symbol a in the same way, and the other argument uniformly at random in [1..|s|a].
Finally, for access we generated the position uniformly at random in [1..n]. Note that
the latter choice favors Huffman-shaped wavelet trees, on which we descend to leaf a

Algorithmica (2014) 69:232–268

259

with probability |s|a/n, whereas for rank and select we descend to any leaf with the
same probability.
Let us ﬁrst analyze the case of Simple English, where the alphabet is smaller. Since
σ is 1000 times smaller than n, the O(σ lg n) terms of Huffman-shaped wavelet trees
are not signiﬁcant, and as a result the variant WTRRR reaches the least space, essentially 
nH0(s)+ o(nH0(s)). It is followed by three variants that use similar space:
WTRG (which has an additional O(n)-bit overhead), AP (whose o(nH0(s)) space term
is higher than that of wavelet trees), and WTNPRRR (whose sublinear space term is
of the form o(n lg σ ), that is, uncompressed). The remaining structures, WTNPRG and
GMR, are not compressed and use much more space.

In terms of time, structure AP is faster than all the others except GMR (which
in exchange uses much more space). The exception is on access queries, where as
explained Huffman-shaped wavelet trees, WTRG and WTRRR, are favored and reach
the same performance of AP. In general, the rule is that variants using plain bitmaps
are faster than those using FID compression, and that variants using pointers and
Huffman shape are faster than those without pointers (as the latter need additional
operations to navigate the tree). These differences are smaller on select queries, where
the binary searches dominate most of the time spent.
The Spanish collection has a much larger alphabet: σ is only 100 times smaller
than n. This impacts on the O(σ lg n) bits used by the pointer-based wavelet trees,
and as a result the space of AP, nH0(s)+ o(nH0(s)), is unparalleled. Variants WTRRR
and WTRG use signiﬁcantly more space and are followed, far away, by WTNPRRR,
which has uncompressed redundancy. The uncompressed variants WTNPRG and RG
use signiﬁcantly more space. The times are basically as on Simple English.

This second collection illustrates more clearly that, for large alphabets, our structure 
AP sharply dominates the whole space/time tradeoff. It is only slightly slower
than GMR in some cases, but in exchange it uses half the space. From the wavelet
trees, the most competitive alternative is WTRG, but it always loses to AP. The situation 
is not too different on smaller alphabets (as in Simple English), except that
variant WTRRR uses clearly less space, yet at the expense of doubling the operation
times of AP.

8.4 Intersecting Inverted Lists

An interesting application of rank/select operations on large alphabets was proposed
by Clarke et al. [14], and recently implemented by Arroyuelo et al. [1] using wavelet
trees. The idea is to represent the text collections as a sequence of word tokens (as
done for Simple English and Spanish), use a compressed and rank/select/accesscapable 
sequence representation for them, and use those operations to emulate an
inverted index on the collection, without spending any extra space on storing explicit
inverted lists.
More precisely, given a collection of d documents T1, T2, . . . , Td , we concatenate
them in C = T1T2 ··· Td , and build an auxiliary bitmap b[1..|C|] where we mark the
beginning of each document with a 1. We can provide access to the text of any document 
in the collection via access operations on sequence C (and select on b). In order

260

Algorithmica (2014) 69:232–268

input : C, w, p
output: next document after Tp that contains w
pos ← b.select1(p + 1);
cnt ← C.rankw(pos − 1);
return b.rank1(C.selectw(cnt + 1))
Algorithm 1: Function nextDoc(C, w, p), retrieves the next document after p
containing w. The x axis starts at the entropy of the sequence

input : C, W = w1, w2, . . . , wk
output: documents that contain w1, . . . , wk
sort W by increasing number of occurrences in the collection;
res ← ∅;
p ← nextDoc(C, w1, 0);
while p is valid do
if w2, . . . , wk are contained in p (i.e., p = nextDoc(C, wj , p − 1) for
2 ≤ j ≤ k) then
Add p to res;
p ← nextDoc(C, w1, p)

end
else

end

Let wj be the ﬁrst word not contained in p;
p ← nextDoc(C, w1,nextDoc(C, wj , p − 1))

end
return res

Algorithm 2: Retrieving the documents where all w1, . . . , wk appear

to emulate the inverted list of a given term w, we just need to list all the distinct documents 
where w occurs. This is achieved by iterating on procedure nextDoc(C, w, p)
of Algorithm 1 (called initially with p = 0 and then using the last p value returned).
Algorithm 1 also allows one to test whether a given document contains a term
or not (p contains w iff p = nextDoc(C, w, p − 1)). Using this primitive we implemented 
Algorithm 2, which intersects several lists (i.e., returns the documents where
all the given terms appear) based on the algorithm by Demaine et al. [18]. We tested
this algorithm for both Simple English and Spanish collections, searching for phrases
extracted at random from the collection. We considered phrases of lengths 2 to 16.
We averaged the results over 1,000 queries. As all the results were quite similar, we
only show the cases of 2 and 6 words. We tested the same structures as in Sect. 8.3.
Figure 3 shows the results obtained by the different structures. For space, of
course, the results are as before: AP is the best on Spanish and is outperformed by
WTRRR on Simple English. With respect to time, we observe that Huffman-shaped
wavelet trees are favored compared to the random rank and select queries of Sect. 8.3.
The reason is that the queries in this application, at least in the way we have generAlgorithmica 
(2014) 69:232–268

261

Fig. 3 Results for intersection queries. The x axis starts at the entropy of the sequence

ated them, do not distribute uniformly at random: the symbols for rank and select are
chosen according to their probability in the text, which favors Huffman-shaped trees.
As a result, structures WTRG perform similarly to AP in time, whereas WTRRR is less
than twice as slow.

8.5 Self-Indexes

A second application of the sequence operations on large alphabets was explored
by Fariña et al. [19]. The idea is to take a self-index [48] designed for text composed 
of characters, and apply it to a word-tokenized text, in order to carry out wordlevel 
searches on natural language texts. This requires less space and time than the
character-based indexes and competes successfully with word-addressing inverted
indexes. One of the variants they explore is to build an FM-index [21, 22] on words
[15]. The FM-index represents the Burrows-Wheeler transform (BWT) [12] sbwt of s.
Using rank and access operations on sbwt the FM-index can, among other operations,
count the number of occurrences of a pattern p[1..k] (in our case, a phrase of k words)
in s[1..n]. This requires O(k) applications of rank and access on sbwt. A self-index
is also able to retrieve any passage of the original sequence s.

We implemented the word-based FM-index with the same structures measured so
far, plus a new variant called APRRR. This is a version of AP where the bitmaps of the
wavelet tree of t are represented using FIDs [55]. The reason is that it was proved [42]

262

Algorithmica (2014) 69:232–268

Fig. 4 Time for counting queries on word-based FM-indexes. The vertical line marks the zero-order
entropy of the sequences; remember that some schemes achieve high-order entropy spaces

that the wavelet tree of sbwt, if the bitmaps are represented using Raman et al.’s FID
[55], achieves space nHk(s) + o(n lg σ ). Since the wavelet tree t of sub-alphabets of
sbwt is a coarsened version of that of sbwt, we expect it to take advantage of Raman
et al.’s representation.

We extracted phrases at random text positions, of lengths 2 to 16, and counted their
number of occurrences using the FM-index. We averaged the results over 100,000
searches. As the results are similar for all lengths, we show the results for lengths 2
and 8. Figure 4 shows the time/space tradeoff obtained.

Conﬁrming the theoretical results [42], the versions using compressed bitmaps
require much less space than the other alternatives. In particular, APRRR uses much
less space than AP, especially on Simple English. In this text the least space is reached
by WTRRR. On Spanish, instead, the O(σ lg n) bits of Huffman-shaped wavelet trees
become relevant and the least space is achieved by APRRR, closely followed by AP
and WTNPRRR. The space/time tradeoff is dominated by APRRR and AP, the two
variants of our structure.

8.6 Navigating Graphs

Finally, our last application scenario is the compact representation of graphs. Let
G = (V , E) be a directed graph. If we concatenate the adjacency lists of the nodes,

Algorithmica (2014) 69:232–268

263

Table 4 Description of the Web crawls considered

Name

Nodes

Edges

Plain adj. list (bits per edge)

EU (EU-2005)
In (Indochina-2002)

862,664
7,414,866

19,235,140
194,109,311

20.81
23.73

the result is a sequence s[1..|E|] over an alphabet of size |V|. If we add a bitmap
b[1..|E|] that marks with a 1 the beginning of the lists, it is very easy to retrieve the
adjacency list of any node v ∈ V , that is, its neighbors, with one select operation on
b followed by one access operation on s per neighbor retrieved.10

It is not hard to reach this space with a classical graph representation. However,
classical representations do not allow one to retrieve efﬁciently the reverse neighbors
of v, that is, the nodes that point to it. The classical solution is to double the space
to represent the transposed graph. Our sequence representation, however, allows us
to retrieve the reverse neighbors using select operations on s, much as Algorithm 1
retrieves the documents where a term w appears: our “documents” are the adjacency
lists of the nodes, and the document identiﬁer is the node v ∈ V that points to the
desired node. Similarly, it is possible to determine whether a given node v points to
, which is not an easy operation with classical adjacency lists. This
a given node v
idea has not only been used in this simple form [15], but also in more sophisticated
scenarios where it was combined with grammar compression of the adjacency lists, or
with other transformations, to compress Web graphs and social networks [16, 17, 37].
For this experiment we used two crawls obtained from the well-known WebGraph
project.11 The main characteristics of these crawls are shown in Table 4. Note that
the alphabets are comparatively much larger than on documents, just around 22–26
times smaller than the sequence length.

(cid:4)

Figure 5 shows the results obtained. The nodes are sorted alphabetically by URL.
A well-known property of Web graphs [10] is that nodes tend to point to other nodes
of the same domain. This property turns into substrings of nearby symbols in the
sequence, and this turns into runs of 0 s or 1 s in the bitmaps of the wavelet trees. This
makes variants like WTNPRRR very competitive in space, whereas APRRR does not
beneﬁt so much. The reason is that the partitioning into classes reorders the symbols,
and the property is lost. Note that variant WTRRR does not perform well in space,
since the number of nodes is too large for a pointer-based tree to be advantageous.
For the same reason, even WTRG uses more space than GMR.12 Overall, we note that
our variants largely dominate the space/time tradeoff, except that WTNPRRR uses less
space (but much more time).

10Note that this works well as long as each node points to at least one node. We solve this problem by
keeping an additional bitmap marking the nodes whose list is not empty.
11http://law.dsi.unimi.it.
12Note that WTRRR is almost 50 % larger than WTRG. This is because the former is a more complex
structure and requires a larger (constant) number of pointers to be represented. Multiplying by the σ nodes
of the Huffman-shaped wavelet tree makes a signiﬁcant difference when the alphabet is so large.

264

Algorithmica (2014) 69:232–268

Fig. 5 Performance on Web graphs, to retrieve direct and reverse neighbors. The vertical line marks the
bits per edge required by a plain adjacency list representation

9 Conclusions and Future Work

We have presented the ﬁrst zero-order compressed representation of sequences supporting 
queries access, rank, and select in loglogarithmic time, so that the redundancy 
of the compressed representation is also compressed. That is, our space for sequence 
s[1..n] over alphabet [1..σ] is nH0(s)+ o(n)(H0(s)+ 1) instead of the usual
nH0(s) + o(n lg σ ) bits. This is very important in many practical applications where
the data is so highly compressible that a redundancy of o(n lg σ ) bits would dominate 
the overall space. While there exist representations using even nH0(s) + o(n)
bits, ours is the ﬁrst one supporting the operations in time O(lg lg σ ) while breaking 
the o(n lg σ ) redundancy barrier. Moreover, our time complexities are adaptive
to the compressibility of the sequence, reaching average times O(lgH0(s)) under
reasonable assumptions. We have given various byproducts of the result, where the
compressed-redundancy property carries over representations of text indexes, permutations,
 functions, binary relations, and so on. It is likely that still other data structures
can beneﬁt from our compressed-redundancy representation. Finally, we have shown
experimentally that our representation is highly practical, on large alphabets, both in
synthetic and real-life application scenarios.

On the other hand, various interesting challenges on sequence representations remain 
open:

Algorithmica (2014) 69:232–268

265

1. Use nHk(s)+o(n)(Hk(s)+1) bits of space, rather than nHk(s)+o(n lg σ ) [4, 33]
or our nH0(s) + o(n)(H0(s) + 1) bits, while still supporting the queries access,
rank, and select efﬁciently.
2. Remove the o(nH0(s)) term from the redundancy while retaining loglogarithmic
query times. Golynski et al. [29] have achieved nH0(s) + o(n) bits of space, but
the time complexities are exponentially higher on large alphabets, O(1 + lg σ
lg lg n ).
3. Lower the o(n) redundancy term, which may be signiﬁcant on highly compressible 
sequences. Our o(n) redundancy is indeed O(
lg lg lg n ). That of Golynski et
al. [29], o( n lg σ
lg n ), is more attractive, at least for small alphabets. Moreover, for the
binary case, P˘atra¸scu [53] obtained O( n
lgc n ) for any constant c, and this is likely
to carry over multiary wavelet trees.

n

After the publication of the conference version of this paper, Belazzougui and
Navarro [8] achieved a different tradeoff for one of our byproducts (Theorem 5).
By spending O(n) further bits, they completely removed the terms dependent on
σ in all time complexities, achieving O(m), O(lg n) and O(r − l + lg n) times for
counting, locating and extracting, respectively. Their technique is based on monotone 
minimum perfect hash functions (mmphfs), which can also be used to improve
some of our results on permutations, for example obtaining constant time for query
π(i) in Theorem 6 and thus improving all the derived results.13 This is just one
example of how lively current research is on this fundamental problem. Another
example is the large amount of recent work attempting to close the gap between
lower and upper bounds when taking into account compression, time and redundancy 
[25–27, 29, 33, 34, 53, 54]. Very recently, Belazzougui and Navarro [9] proved
a lower bound of Ω(lg lg σ
lg w ) for operation rank on a RAM machine of word size w,
O(1)), and achieved this time within
which holds for any space of the form O(nw
O(n lg σ ) bits of space. Then, making use of the results we present in this paper, they
reduced the space to nH0(s) + o(nH0(s)) + o(n) bits. This illustrates how our technique 
can be easily used to move from linear-space to compressed-redundancy-space
sequence representations.

To conclude, it is worth mentioning Navarro and Nekrich’s recent result [49] on
optimal representations of dynamic sequences, where in addition to the three operations 
we consider in this paper, one can insert and delete symbols at arbitrary positions.
 They obtain the optimal time O(log n/ log log n) for all the operations within
essentially nH0(s) + O(n) bits of space. Using the alphabet partitioning idea, combining 
structures different from those we used here, is the key to remove any dependence 
on the alphabet size from the query times, to remove o(n lg σ ) terms from the
space, and to handle unbounded alphabets. This shows how the alphabet partitioning
concept may have many more applications than those we are able to envision at this
moment.

Acknowledgements We thank Djamal Belazzougui for helpful comments on a draft of this paper, and
Meg Gagie for righting our grammar.

13Djamal Belazzougui, personal communication.

266

References

Algorithmica (2014) 69:232–268

1. Arroyuelo, D., González, S., Oyarzún, M.: Compressed self-indices supporting conjunctive queries on
document collections. In: Proc. 17th International Symposium on String Processing and Information
Retrieval (SPIRE), pp. 43–54 (2010)

2. Barbay, J., Claude, F., Navarro, G.: Compact rich-functional binary relation representations. In: Proc.
9th Latin American Symposium on Theoretical Informatics (LATIN). LNCS, vol. 6034, pp. 170–183
(2010)

3. Barbay, J., Golynski, A., Munro, J.I., Rao, S.S.: Adaptive searching in succinctly encoded binary

relations and tree-structured documents. Theor. Comput. Sci. 387(3), 284–297 (2007)

4. Barbay, J., He, M., Munro, J.I., Rao, S.S.: Succinct indexes for strings, binary relations and multilabeled 
trees. ACM Trans. Algorithms 7(4), 52 (2011)

5. Barbay, J., López-Ortiz, A., Lu, T., Salinger, A.: An experimental investigation of set intersection

algorithms for text searching. ACM J. Exp. Algorithmics 14(3), 7 (2009)

6. Barbay, J., Navarro, G.: Compressed representations of permutations, and applications. In: Proc. 26th

Symposium on Theoretical Aspects of Computer Science (STACS), pp. 111–122 (2009)

7. Barbay, J., Navarro, G.: On compressing permutations and adaptive sorting. CoRR (2011). 1108.

4408v1

8. Belazzougui, D., Navarro, G.: Alphabet-independent compressed text indexing. In: Proc. 19th Annual

European Symposium on Algorithms (ESA). LNCS, vol. 6942, pp. 748–759 (2011)

9. Belazzougui, D., Navarro, G.: New lower and upper bounds for representing sequences. In: Proc. 20th

Annual European Symposium on Algorithms (ESA). LNCS, vol. 7501, pp. 181–192 (2012)

10. Boldi, P., Vigna, S.: The WebGraph framework I: compression techniques. In: Proc. 13th World Wide

Web Conference (WWW), pp. 595–602 (2004)

11. Brisaboa, N., Luaces, M., Navarro, G., Seco, D.: A new point access method based on wavelet trees.
In: Proc. 3rd International Workshop on Semantic and Conceptual Issues in GIS (SeCoGIS). LNCS,
vol. 5833, pp. 297–306 (2009)

12. Burrows, M., Wheeler, D.: A block sorting lossless data compression algorithm. Technical Report

124, Digital Equipment Corporation (1994)

13. Clark, D.: Compact Pat Trees. Ph.D. Thesis, University of Waterloo, Canada (1996)
14. Clarke, C., Cormack, G., Tudhope, E.: Relevance ranking for one to three term queries. In: Proc. 5th

International Conference on Computer-Assisted Information Retrieval (RIAO), pp. 388–401 (1997)

15. Claude, F., Navarro, G.: Practical rank/select queries over arbitrary sequences. In: Proc. 15th International 
Symposium on String Processing and Information Retrieval (SPIRE), pp. 176–187 (2008)

16. Claude, F., Navarro, G.: Extended compact web graph representations. In: Elomaa, T., Mannila, H.,
Orponen, P. (eds.) Algorithms and Applications (Ukkonen Festschrift). LNCS, vol. 6060, pp. 77–91.
Springer, Berlin (2010)

17. Claude, F., Navarro, G.: Fast and compact web graph representations. ACM Trans. Web 4(4), 16

(2010)

18. Demaine, E., López-Ortiz, A., Munro, J.I.: Adaptive set intersections, unions, and differences. In:

Proc. 11th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 743–752 (2000)

19. Fariña, A., Brisaboa, N., Navarro, G., Claude, F., Places, A., Rodríguez, E.: Word-based self-indexes

for natural language text. ACM Trans. Inf. Syst. 30(1), 1 (2012)

20. Ferragina, P., Luccio, F., Manzini, G., Muthukrishnan, S.: Compressing and indexing labeled trees,

with applications. J. ACM 57(1), 4 (2009)

21. Ferragina, P., Manzini, G.: Indexing compressed texts. J. ACM 52(4), 552–581 (2005)
22. Ferragina, P., Manzini, G., Mäkinen, V., Navarro, G.: Compressed representations of sequences and

full-text indexes. ACM Trans. Algorithms 3(2), 20 (2007)

23. Ferragina, P., Venturini, R.: A simple storage scheme for strings achieving entropy bounds. Theor.

Comput. Sci. 372(1), 115–121 (2007)

24. Gagie, T., Nekrich, Y.: Worst-case optimal adaptive preﬁx coding. In: Proc. 11th International Symposium 
on Algorithms and Data Structures (WADS). LNCS, vol. 5664, pp. 315–326 (2009)

25. Golynski, A.: Optimal lower bounds for rank and select indexes. Theor. Comput. Sci. 387(3), 348–359

(2007)

26. Golynski, A.: Cell probe lower bounds for succinct data structures. In: Proc. 20th Annual ACM-SIAM

Symposium on Discrete Algorithms (SODA), pp. 625–634 (2009)

27. Golynski, A., Grossi, R., Gupta, A., Raman, R., Srinivasa Rao, S.: On the size of succinct indices.
In: Proc. 15th Annual European Symposium on Algorithms (ESA). LNCS, vol. 4698, pp. 371–382
(2007)

Algorithmica (2014) 69:232–268

267

28. Golynski, A., Munro, J.I., Rao, S.S.: Rank/select operations on large alphabets: a tool for text indexing.
 In: Proc. 17th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 368–373
(2006)

29. Golynski, A., Raman, R., Rao, S.: On the redundancy of succinct data structures. In: Proc. 11th Scandinavian 
Workshop on Algorithm Theory (SWAT). LNCS, vol. 5124, pp. 148–159 (2008)

30. González, R., Grabowski, Sz., Mäkinen, V., Navarro, G.: Practical implementation of rank and select
queries. In: Proc. 4th Workshop on Efﬁcient and Experimental Algorithms (WEA), pp. 27–38 (2005).
Posters

31. González, R., Navarro, G.: Statistical encoding of succinct data structures. In: Proc. 17th Annual

Symposium on Combinatorial Pattern Matching (CPM), pp. 294–305 (2006)

32. Grossi, R., Gupta, A., Vitter, J.: High-order entropy-compressed text indexes. In: Proc. 14th Annual

ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 841–850 (2003)

33. Grossi, R., Orlandi, A., Raman, R.: Optimal trade-offs for succinct string indexes. In: Proc. 37th

International Colloquim on Automata, Languages and Programming (ICALP), pp. 678–689 (2010)

34. Grossi, R., Orlandi, A., Raman, R., Srinivasa Rao, S.: More haste, less waste: lowering the redundancy
in fully indexable dictionaries. In: Proc. 26th Symposium on Theoretical Aspects of Computer Science
(STACS), pp. 517–528 (2009)

35. Grossi, R., Vitter, J.: Compressed sufﬁx arrays and sufﬁx trees with applications to text indexing and

string matching. SIAM J. Comput. 35(2), 378–407 (2006)

36. Haskel, B., Puri, A., Netravali, A.: Digital Video: an Introduction to MPEG-2. Chapman & Hall,

London (1997)

37. Hernández, C., Navarro, G.: Compression of web and social graphs supporting neighbor and community 
queries. In: Proc. 5th ACM Workshop on Social Network Mining and Analysis (SNA-KDD).
ACM, New York (2011)

38. Hreinsson, J.B., Krøyer, M., Pagh, R.: Storing a compressed function with constant time access. In:

Proc. 17th European Symposium on Algorithms (ESA), pp. 730–741 (2009)

39. Huffman, D.: A method for the construction of minimum-redundancy codes. Proc. IRE 40(9), 1090–

1101 (1952)

40. Levcopoulos, C., Petersson, O.: Sorting shufﬂed monotone sequences. Inf. Comput. 112(1), 37–50

(1994)

41. Mäkinen, V., Navarro, G.: Rank and select revisited and extended. Theor. Comput. Sci. 387(3), 332–

347 (2007)

42. Mäkinen, V., Navarro, G.: Dynamic entropy-compressed sequences and full-text indexes. ACM Trans.

Algorithms 4(3), 32 (2008)

43. Manzini, G.: An analysis of the Burrows-Wheeler transform. J. ACM 48(3), 407–430 (2001)
44. Mehlhorn, K.: Sorting presorted ﬁles. In: Proc. 4th GI-Conference on Theoretical Computer Science.

LNCS, vol. 67, pp. 199–212 (1979)

45. Moffat, A., Turpin, A.: On the implementation of minimum-redundancy preﬁx codes. IEEE Trans.

Commun. 45(10), 1200–1207 (1997)

46. Munro, I.: Tables. In: Proc. 16th Conference on Foundations of Software Technology and Theoretical

Computer Science (FSTTCS). LNCS, vol. 1180, pp. 37–42 (1996)

47. Munro, I., Raman, R., Raman, V., Rao, S.S.: Succinct representations of permutations and functions.

Theor. Comput. Sci. 438, 74–88 (2012)

48. Navarro, G., Mäkinen, V.: Compressed full-text indexes. ACM Comput. Surv. 39(1), 2 (2007)
49. Navarro, G., Nekrich, Y.: Optimal dynamic sequence representations. In: Proc. 24th Annual ACMSIAM 
Symposium on Discrete Algorithms (SODA) (2013, to appear)

50. Okanohara, D., Sadakane, K.: Practical entropy-compressed rank/select dictionary. In: Proc. 10th

Workshop on Algorithm Engineering and Experiments (ALENEX), pp. 60–70 (2007)

51. Pearlman, W., Islam, A., Nagaraj, N., Said, A.: Efﬁcient, low-complexity image coding with a setpartitioning 
embedded block coder. IEEE Trans. Circuits Syst. Video Technol. 14(11), 1219–1235
(2004)

52. Pennebaker, W., Mitchell, J.: JPEG: Still Image Data Compression Standard. Van Nostrand-Reinhold,

New York (1992)

53. P˘atra¸scu, M.: Succincter. In: Proc. 49th Annual IEEE Symposium on Foundations of Computer Science 
(FOCS), pp. 305–313 (2008)

54. P˘atra¸scu, M.: A lower bound for succinct rank queries. CoRR (2009). arXiv:0907.1103v1 [cs.DS]
55. Raman, R., Raman, V., Rao, S.: Succinct indexable dictionaries with applications to encoding k-ary

trees, preﬁx sums and multisets. ACM Trans. Algorithms 3(4), 43 (2007)

268

Algorithmica (2014) 69:232–268

56. Russo, L., Navarro, G., Oliveira, A., Morales, P.: Approximate string matching with compressed indexes.
 Algorithms 2(3), 1105–1136 (2009)

57. Sadakane, K.: New text indexing functionalities of the compressed sufﬁx arrays. J. Algorithms 48(2),

294–313 (2003)

58. Sadakane, K., Grossi, R.: Squeezing succinct data structures into entropy bounds. In: Proc. 17th Annual 
ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 1230–1239 (2006)

59. Said, A.: Efﬁcient alphabet partitioning algorithms for low-complexity entropy coding. In: Proc. 15th

Data Compression Conference (DCC), pp. 193–202 (2005)

60. Tarjan, R.E., van Leeuwen, J.: Worst-case analysis of set union algorithms. J. ACM 31(2), 245–281

(1984)

61. Witten, I., Moffat, A., Bell, T.: Managing Gigabytes, 2nd edn. Morgan Kaufmann, San Mateo (1999)

